{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ryankirkland/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords,wordnet\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/total_reviews.csv')\n",
    "cleaned = pd.read_csv('../data/cleaned_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B08267BBJT', 'B08268F6XN', 'B08267X3LH', 'B079JFK22D',\n",
       "       'B07QW531W2', 'B07TJTQDYG', 'B085HB8QVX', 'B07R2KK5P7',\n",
       "       'B07Y475MD3', 'B07F27PK2M', 'B085FZFVQV', 'B072R2SWXX',\n",
       "       'B0828KRQZ3', 'B0821ZNWKW', 'B07QV15B3W', 'B07MWYYDTM',\n",
       "       'B07RSJMS76', 'B07D1LMMDD', 'B0855TM65T', 'B07NTXYFBV',\n",
       "       'B086L3Q8YX', 'B085FT4YR1', 'B085XT3GTW', 'B07P9XZPYG',\n",
       "       'B0824WB5ST', 'B07HQ7QV7W', 'B083ZMYF55', 'B07FQD7PZ5',\n",
       "       'B07TWDR7VJ', 'B085DV7VZK', 'B086GTFGPP', 'B07Q6PZ2F4',\n",
       "       'B082W54KQK'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['asin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(pos_tag):\n",
    "\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ \n",
    "\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB \n",
    "\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    \n",
    "    else:\n",
    "        return wordnet.NOUN # default, return wordnet tag \"NOUN\"\n",
    "\n",
    "#Create a function to lemmatize tokens in the reviews\n",
    "def lemmatized_tokens(text):\n",
    "        text = text.lower()\n",
    "        pattern = r'\\b[a-zA-Z]{3,}\\b'                 \n",
    "        tokens = nltk.regexp_tokenize(text, pattern) # tokenize the text\n",
    "        tagged_tokens = nltk.pos_tag(tokens)  # a list of tuples (word, pos_tag)\n",
    "          \n",
    "        stop_words = stopwords.words('english')\n",
    "        new_stopwords = []  #customize extra stop_words\n",
    "        stop_words.extend(new_stopwords)\n",
    "        stop_words = set(stop_words)\n",
    "        \n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        # get lemmatized tokens                             #call function \"get_wordnet_pos\"\n",
    "        lemmatized_words=[wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(tag)) \n",
    "                  # tagged_tokens is a list of tuples (word, tag)\n",
    "                  for (word, tag) in tagged_tokens \\\n",
    "                  # remove stop words\n",
    "                  if word not in stop_words and \\\n",
    "                  # remove punctuations\n",
    "                  word not in string.punctuation]\n",
    "\n",
    "        return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = cleaned.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>product</th>\n",
       "      <th>date</th>\n",
       "      <th>verified</th>\n",
       "      <th>title</th>\n",
       "      <th>desc</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>rating</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>month_year</th>\n",
       "      <th>title_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B08267BBJT</td>\n",
       "      <td>AAA</td>\n",
       "      <td>2020-08-11</td>\n",
       "      <td>Verified Purchase</td>\n",
       "      <td>Didn't work, then worked, now don't work again</td>\n",
       "      <td>All I got in terms of use out of these batter...</td>\n",
       "      <td>Jasmine Carroll</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-08</td>\n",
       "      <td>Didn't work, then worked, now don't work again...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B08267BBJT</td>\n",
       "      <td>AAA</td>\n",
       "      <td>2020-07-30</td>\n",
       "      <td>Verified Purchase</td>\n",
       "      <td>These absolutely suck</td>\n",
       "      <td>I bought these for a wall mounted magnifying ...</td>\n",
       "      <td>Ashlee M.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-07</td>\n",
       "      <td>These absolutely suck I bought these for a wal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B08268F6XN</td>\n",
       "      <td>AA</td>\n",
       "      <td>2020-07-19</td>\n",
       "      <td>Verified Purchase</td>\n",
       "      <td>longer lasting battery for remote controller!!</td>\n",
       "      <td>i like the constant voltage and hopefully it ...</td>\n",
       "      <td>ARCHANGEL TROY</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-07</td>\n",
       "      <td>longer lasting battery for remote controller!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B08267BBJT</td>\n",
       "      <td>AAA</td>\n",
       "      <td>2020-07-18</td>\n",
       "      <td>Verified Purchase</td>\n",
       "      <td>Minimal plastic in packaging.</td>\n",
       "      <td>Just received these today, but I’m reviewing ...</td>\n",
       "      <td>ira</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-07</td>\n",
       "      <td>Minimal plastic in packaging. Just received th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B08267BBJT</td>\n",
       "      <td>AAA</td>\n",
       "      <td>2020-07-17</td>\n",
       "      <td>Verified Purchase</td>\n",
       "      <td>Not long enough battery life for a night hike</td>\n",
       "      <td>Shuts off suddenly in headlamp</td>\n",
       "      <td>T</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-07</td>\n",
       "      <td>Not long enough battery life for a night hike ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin product        date           verified  \\\n",
       "0  B08267BBJT     AAA  2020-08-11  Verified Purchase   \n",
       "1  B08267BBJT     AAA  2020-07-30  Verified Purchase   \n",
       "2  B08268F6XN      AA  2020-07-19  Verified Purchase   \n",
       "3  B08267BBJT     AAA  2020-07-18  Verified Purchase   \n",
       "4  B08267BBJT     AAA  2020-07-17  Verified Purchase   \n",
       "\n",
       "                                            title  \\\n",
       "0  Didn't work, then worked, now don't work again   \n",
       "1                           These absolutely suck   \n",
       "2  longer lasting battery for remote controller!!   \n",
       "3                   Minimal plastic in packaging.   \n",
       "4   Not long enough battery life for a night hike   \n",
       "\n",
       "                                                desc    reviewer_name  rating  \\\n",
       "0   All I got in terms of use out of these batter...  Jasmine Carroll     1.0   \n",
       "1   I bought these for a wall mounted magnifying ...        Ashlee M.     1.0   \n",
       "2   i like the constant voltage and hopefully it ...   ARCHANGEL TROY     5.0   \n",
       "3   Just received these today, but I’m reviewing ...              ira     5.0   \n",
       "4                     Shuts off suddenly in headlamp                T     3.0   \n",
       "\n",
       "   month  year month_year                                         title_desc  \n",
       "0      8  2020    2020-08  Didn't work, then worked, now don't work again...  \n",
       "1      7  2020    2020-07  These absolutely suck I bought these for a wal...  \n",
       "2      7  2020    2020-07  longer lasting battery for remote controller!!...  \n",
       "3      7  2020    2020-07  Minimal plastic in packaging. Just received th...  \n",
       "4      7  2020    2020-07  Not long enough battery life for a night hike ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt = cleaned.loc[0, 'title_desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmed_test = lemmatized_tokens(test_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['work',\n",
       " 'work',\n",
       " 'work',\n",
       " 'get',\n",
       " 'term',\n",
       " 'use',\n",
       " 'battery',\n",
       " 'three',\n",
       " 'day',\n",
       " 'use',\n",
       " 'two',\n",
       " 'additional',\n",
       " 'success',\n",
       " 'buy',\n",
       " 'bleed',\n",
       " 'aaa',\n",
       " 'battery',\n",
       " 'hop',\n",
       " 'compact',\n",
       " 'design',\n",
       " 'would',\n",
       " 'better',\n",
       " 'something',\n",
       " 'bulky',\n",
       " 'right',\n",
       " 'box',\n",
       " 'charge',\n",
       " 'light',\n",
       " 'green',\n",
       " 'indicate',\n",
       " 'fully',\n",
       " 'charge',\n",
       " 'try',\n",
       " 'use',\n",
       " 'couple',\n",
       " 'device',\n",
       " 'luck',\n",
       " 'go',\n",
       " 'return',\n",
       " 'friend',\n",
       " 'suggest',\n",
       " 'switch',\n",
       " 'charge',\n",
       " 'extension',\n",
       " 'cord',\n",
       " 'directly',\n",
       " 'wall',\n",
       " 'socket',\n",
       " 'think',\n",
       " 'trick',\n",
       " 'even',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'silly',\n",
       " 'try',\n",
       " 'battery',\n",
       " 'device',\n",
       " 'let',\n",
       " 'charge',\n",
       " 'overnight',\n",
       " 'plug',\n",
       " 'directly',\n",
       " 'wall',\n",
       " 'socket',\n",
       " 'work',\n",
       " 'work',\n",
       " 'well',\n",
       " 'three',\n",
       " 'day',\n",
       " 'later',\n",
       " 'device',\n",
       " 'stop',\n",
       " 'work',\n",
       " 'middle',\n",
       " 'high',\n",
       " 'power',\n",
       " 'usage',\n",
       " 'hair',\n",
       " 'trimmer',\n",
       " 'swap',\n",
       " 'battery',\n",
       " 'two',\n",
       " 'charge',\n",
       " 'entire',\n",
       " 'time',\n",
       " 'work',\n",
       " 'go',\n",
       " 'buy',\n",
       " 'regular',\n",
       " 'aaa',\n",
       " 'battery',\n",
       " 'device',\n",
       " 'go',\n",
       " 'back',\n",
       " 'work',\n",
       " 'fine',\n",
       " 'try',\n",
       " 'battery',\n",
       " 'device',\n",
       " 'work',\n",
       " 'seem',\n",
       " 'work',\n",
       " 'enough',\n",
       " 'convince',\n",
       " 'work',\n",
       " 'stop',\n",
       " 'work',\n",
       " 'often',\n",
       " 'leave',\n",
       " 'feedback',\n",
       " 'product',\n",
       " 'felt',\n",
       " 'important',\n",
       " 'say',\n",
       " 'would',\n",
       " 'recommend',\n",
       " 'anyone',\n",
       " 'buy',\n",
       " 'battery',\n",
       " 'totally',\n",
       " 'faulty',\n",
       " 'least',\n",
       " 'completely',\n",
       " 'inconsistent',\n",
       " 'good',\n",
       " 'buying',\n",
       " 'disposable']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to build the optimal LDA model\n",
    "def optimal_lda_model(df, review_colname):\n",
    "    '''\n",
    "    INPUTS:\n",
    "        df_review - dataframe that contains the reviews\n",
    "        review_colname: name of column that contains reviews\n",
    "        \n",
    "    OUTPUTS:\n",
    "        lda_tfidf - Latent Dirichlet Allocation (LDA) model\n",
    "        dtm_tfidf - document-term matrix in the tfidf format\n",
    "        tfidf_vectorizer - word frequency in the reviews\n",
    "        A graph comparing LDA Model Performance Scores with different params\n",
    "    '''\n",
    "    docs_raw = df[review_colname].tolist()\n",
    "\n",
    "    #************   Step 1: Convert to document-term matrix   ************#\n",
    "\n",
    "    #Transform text to vector form using the vectorizer object \n",
    "    tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                    stop_words = 'english',\n",
    "                                    lowercase = True,\n",
    "                                    token_pattern = r'\\b[a-zA-Z]{3,}\\b', # num chars > 3 to avoid some meaningless words\n",
    "                                    max_df = 0.8,                        # discard words that appear in > 90% of the reviews\n",
    "                                    min_df = 10)                         # discard words that appear in < 10 reviews    \n",
    "\n",
    "    #apply transformation\n",
    "    tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())\n",
    "\n",
    "    #convert to document-term matrix\n",
    "    dtm_tfidf = tfidf_vectorizer.fit_transform(docs_raw)  \n",
    "\n",
    "    print(\"The shape of the tfidf is {}, meaning that there are {} {} and {} tokens made through the filtering process.\".\\\n",
    "              format(dtm_tfidf.shape,dtm_tfidf.shape[0], review_colname, dtm_tfidf.shape[1]))\n",
    "\n",
    "    \n",
    "    #*******   Step 2: GridSearch & parameter tuning to find the optimal LDA model   *******#\n",
    "\n",
    "    # Define Search Param\n",
    "    search_params = {'n_components': [5, 10, 15, 20, 25, 30], \n",
    "                     'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "    # Init the Model\n",
    "    lda = LatentDirichletAllocation()\n",
    "\n",
    "    # Init Grid Search Class\n",
    "    model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "    # Do the Grid Search\n",
    "    model.fit(dtm_tfidf)\n",
    "\n",
    "\n",
    "    #*****  Step 3: Output the optimal lda model and its parameters  *****#\n",
    "\n",
    "    # Best Model\n",
    "    best_lda_model = model.best_estimator_\n",
    "\n",
    "    # Model Parameters\n",
    "    print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "    # Log Likelihood Score: Higher the better\n",
    "    print(\"Model Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "    # Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "    print(\"Model Perplexity: \", best_lda_model.perplexity(dtm_tfidf))\n",
    "\n",
    "\n",
    "    #***********   Step 4: Compare LDA Model Performance Scores   ***********#\n",
    "\n",
    "    #Get Log Likelyhoods from Grid Search Output\n",
    "    gscore=model.fit(dtm_tfidf).cv_results_\n",
    "    n_topics = [5, 10, 15, 20, 25, 30]\n",
    "\n",
    "    log_likelyhoods_5 = [gscore['mean_test_score'][gscore['params'].index(v)] for v in gscore['params'] if v['learning_decay']==0.5]\n",
    "    log_likelyhoods_7 = [gscore['mean_test_score'][gscore['params'].index(v)] for v in gscore['params'] if v['learning_decay']==0.7]\n",
    "    log_likelyhoods_9 = [gscore['mean_test_score'][gscore['params'].index(v)] for v in gscore['params'] if v['learning_decay']==0.9]\n",
    "\n",
    "    # Show graph\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
    "    plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
    "    plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
    "    plt.title(\"Choosing Optimal LDA Model\")\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Log Likelyhood Scores\")\n",
    "    plt.legend(title='Learning decay', loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return gscore, best_lda_model, dtm_tfidf, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryankirkland/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1798: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. <class 'numpy.int64'> 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the tfidf is (4745, 1120), meaning that there are 4745 title_desc and 1120 tokens made through the filtering process.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-837b8c26620f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_lda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtm_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimal_lda_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title_desc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-fed7ae4b58a6>\u001b[0m in \u001b[0;36moptimal_lda_model\u001b[0;34m(df, review_colname)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Do the Grid Search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtm_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    713\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 715\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    578\u001b[0m                     \u001b[0;31m# batch update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                     self._em_step(X, total_samples=n_samples,\n\u001b[0;32m--> 580\u001b[0;31m                                   batch_update=True, parallel=parallel)\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0;31m# check perplexity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36m_em_step\u001b[0;34m(self, X, total_samples, batch_update, parallel)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;31m# E-step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         _, suff_stats = self._e_step(X, cal_sstats=True, random_init=True,\n\u001b[0;32m--> 448\u001b[0;31m                                      parallel=parallel)\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;31m# M-step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36m_e_step\u001b[0;34m(self, X, cal_sstats, random_init, parallel)\u001b[0m\n\u001b[1;32m    399\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_change_tol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcal_sstats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                                               random_state)\n\u001b[0;32m--> 401\u001b[0;31m             for idx_slice in gen_even_slices(X.shape[0], n_jobs))\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;31m# merge result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36m_update_doc_distribution\u001b[0;34m(X, exp_topic_word_distr, doc_topic_prior, max_iters, mean_change_tol, cal_sstats, random_state)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;31m# Note: adds doc_topic_prior to doc_topic_d, in-place.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             _dirichlet_expectation_1d(doc_topic_d, doc_topic_prior,\n\u001b[0;32m--> 119\u001b[0;31m                                       exp_doc_topic_d)\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmean_change\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_d\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmean_change_tol\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gscore, best_lda_model, dtm_tfidf, tfidf_vectorizer = optimal_lda_model(cleaned, 'title_desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = pd.DataFrame(gscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_decay</th>\n",
       "      <th>param_n_components</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.845956</td>\n",
       "      <td>0.104144</td>\n",
       "      <td>0.153028</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_decay': 0.5, 'n_components': 5}</td>\n",
       "      <td>-28488.558606</td>\n",
       "      <td>-28930.989002</td>\n",
       "      <td>-27153.294572</td>\n",
       "      <td>-22561.875711</td>\n",
       "      <td>-22773.777617</td>\n",
       "      <td>-25981.699101</td>\n",
       "      <td>2769.160296</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.790072</td>\n",
       "      <td>0.136663</td>\n",
       "      <td>0.127356</td>\n",
       "      <td>0.009912</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_decay': 0.5, 'n_components': 10}</td>\n",
       "      <td>-33087.416985</td>\n",
       "      <td>-33513.216148</td>\n",
       "      <td>-30551.072069</td>\n",
       "      <td>-25346.915472</td>\n",
       "      <td>-26303.785883</td>\n",
       "      <td>-29760.481311</td>\n",
       "      <td>3382.450596</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.786485</td>\n",
       "      <td>0.221351</td>\n",
       "      <td>0.133783</td>\n",
       "      <td>0.008054</td>\n",
       "      <td>0.5</td>\n",
       "      <td>15</td>\n",
       "      <td>{'learning_decay': 0.5, 'n_components': 15}</td>\n",
       "      <td>-35253.444208</td>\n",
       "      <td>-35915.864000</td>\n",
       "      <td>-34697.528019</td>\n",
       "      <td>-28949.165100</td>\n",
       "      <td>-28021.033834</td>\n",
       "      <td>-32567.407032</td>\n",
       "      <td>3368.250559</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.026420</td>\n",
       "      <td>0.134606</td>\n",
       "      <td>0.138743</td>\n",
       "      <td>0.009025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20</td>\n",
       "      <td>{'learning_decay': 0.5, 'n_components': 20}</td>\n",
       "      <td>-38738.205443</td>\n",
       "      <td>-39143.454406</td>\n",
       "      <td>-37213.094375</td>\n",
       "      <td>-32353.431606</td>\n",
       "      <td>-30823.541876</td>\n",
       "      <td>-35654.345541</td>\n",
       "      <td>3416.034759</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.966580</td>\n",
       "      <td>0.118991</td>\n",
       "      <td>0.137061</td>\n",
       "      <td>0.011771</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25</td>\n",
       "      <td>{'learning_decay': 0.5, 'n_components': 25}</td>\n",
       "      <td>-40890.790108</td>\n",
       "      <td>-42208.995511</td>\n",
       "      <td>-38992.560353</td>\n",
       "      <td>-33969.188965</td>\n",
       "      <td>-33496.305939</td>\n",
       "      <td>-37911.568175</td>\n",
       "      <td>3565.082989</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.834590</td>\n",
       "      <td>0.165225</td>\n",
       "      <td>0.136654</td>\n",
       "      <td>0.008059</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>{'learning_decay': 0.5, 'n_components': 30}</td>\n",
       "      <td>-42312.718053</td>\n",
       "      <td>-42835.708947</td>\n",
       "      <td>-40528.415772</td>\n",
       "      <td>-35837.246247</td>\n",
       "      <td>-36164.727810</td>\n",
       "      <td>-39535.763366</td>\n",
       "      <td>2987.620678</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.020092</td>\n",
       "      <td>0.193916</td>\n",
       "      <td>0.153264</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_decay': 0.7, 'n_components': 5}</td>\n",
       "      <td>-28384.681008</td>\n",
       "      <td>-28500.782516</td>\n",
       "      <td>-27141.818990</td>\n",
       "      <td>-22248.838749</td>\n",
       "      <td>-22571.661941</td>\n",
       "      <td>-25769.556641</td>\n",
       "      <td>2785.805419</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.220248</td>\n",
       "      <td>0.150028</td>\n",
       "      <td>0.149348</td>\n",
       "      <td>0.017156</td>\n",
       "      <td>0.7</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_decay': 0.7, 'n_components': 10}</td>\n",
       "      <td>-32136.400816</td>\n",
       "      <td>-32668.045994</td>\n",
       "      <td>-31263.006276</td>\n",
       "      <td>-25633.120238</td>\n",
       "      <td>-26415.223464</td>\n",
       "      <td>-29623.159358</td>\n",
       "      <td>2982.887194</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.390310</td>\n",
       "      <td>0.393962</td>\n",
       "      <td>0.143945</td>\n",
       "      <td>0.005179</td>\n",
       "      <td>0.7</td>\n",
       "      <td>15</td>\n",
       "      <td>{'learning_decay': 0.7, 'n_components': 15}</td>\n",
       "      <td>-35405.972626</td>\n",
       "      <td>-36358.420192</td>\n",
       "      <td>-33618.448165</td>\n",
       "      <td>-29563.799087</td>\n",
       "      <td>-28930.848376</td>\n",
       "      <td>-32775.497689</td>\n",
       "      <td>3018.729329</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.142307</td>\n",
       "      <td>0.124830</td>\n",
       "      <td>0.137806</td>\n",
       "      <td>0.014752</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20</td>\n",
       "      <td>{'learning_decay': 0.7, 'n_components': 20}</td>\n",
       "      <td>-37413.359244</td>\n",
       "      <td>-38599.003660</td>\n",
       "      <td>-37385.390037</td>\n",
       "      <td>-30739.987513</td>\n",
       "      <td>-31671.982882</td>\n",
       "      <td>-35161.944667</td>\n",
       "      <td>3272.903597</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.848557</td>\n",
       "      <td>0.275192</td>\n",
       "      <td>0.134378</td>\n",
       "      <td>0.007419</td>\n",
       "      <td>0.7</td>\n",
       "      <td>25</td>\n",
       "      <td>{'learning_decay': 0.7, 'n_components': 25}</td>\n",
       "      <td>-41308.243278</td>\n",
       "      <td>-41241.056900</td>\n",
       "      <td>-37776.620317</td>\n",
       "      <td>-33433.514468</td>\n",
       "      <td>-33856.014091</td>\n",
       "      <td>-37523.089811</td>\n",
       "      <td>3417.221520</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.737570</td>\n",
       "      <td>0.152812</td>\n",
       "      <td>0.133738</td>\n",
       "      <td>0.009334</td>\n",
       "      <td>0.7</td>\n",
       "      <td>30</td>\n",
       "      <td>{'learning_decay': 0.7, 'n_components': 30}</td>\n",
       "      <td>-42912.140151</td>\n",
       "      <td>-43850.598288</td>\n",
       "      <td>-40847.950884</td>\n",
       "      <td>-34917.495141</td>\n",
       "      <td>-35919.661601</td>\n",
       "      <td>-39689.569213</td>\n",
       "      <td>3633.892372</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.814275</td>\n",
       "      <td>0.126414</td>\n",
       "      <td>0.152432</td>\n",
       "      <td>0.012295</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_decay': 0.9, 'n_components': 5}</td>\n",
       "      <td>-28233.800754</td>\n",
       "      <td>-28663.377834</td>\n",
       "      <td>-27096.922909</td>\n",
       "      <td>-22279.122570</td>\n",
       "      <td>-22358.029536</td>\n",
       "      <td>-25726.250721</td>\n",
       "      <td>2829.164949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.283745</td>\n",
       "      <td>0.414544</td>\n",
       "      <td>0.144162</td>\n",
       "      <td>0.012471</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_decay': 0.9, 'n_components': 10}</td>\n",
       "      <td>-32388.195499</td>\n",
       "      <td>-32939.353683</td>\n",
       "      <td>-31174.632342</td>\n",
       "      <td>-26850.588824</td>\n",
       "      <td>-26296.863886</td>\n",
       "      <td>-29929.926847</td>\n",
       "      <td>2804.656683</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.894476</td>\n",
       "      <td>0.155976</td>\n",
       "      <td>0.136125</td>\n",
       "      <td>0.009681</td>\n",
       "      <td>0.9</td>\n",
       "      <td>15</td>\n",
       "      <td>{'learning_decay': 0.9, 'n_components': 15}</td>\n",
       "      <td>-34367.585513</td>\n",
       "      <td>-37050.195673</td>\n",
       "      <td>-33863.430047</td>\n",
       "      <td>-29098.199969</td>\n",
       "      <td>-28309.757282</td>\n",
       "      <td>-32537.833697</td>\n",
       "      <td>3321.876528</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.836675</td>\n",
       "      <td>0.124257</td>\n",
       "      <td>0.132903</td>\n",
       "      <td>0.010458</td>\n",
       "      <td>0.9</td>\n",
       "      <td>20</td>\n",
       "      <td>{'learning_decay': 0.9, 'n_components': 20}</td>\n",
       "      <td>-38753.627273</td>\n",
       "      <td>-39044.787415</td>\n",
       "      <td>-36366.257228</td>\n",
       "      <td>-32174.173800</td>\n",
       "      <td>-31393.346284</td>\n",
       "      <td>-35546.438400</td>\n",
       "      <td>3219.222014</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.834491</td>\n",
       "      <td>0.164795</td>\n",
       "      <td>0.133474</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.9</td>\n",
       "      <td>25</td>\n",
       "      <td>{'learning_decay': 0.9, 'n_components': 25}</td>\n",
       "      <td>-40646.687393</td>\n",
       "      <td>-40750.529572</td>\n",
       "      <td>-39331.553379</td>\n",
       "      <td>-32836.979462</td>\n",
       "      <td>-32857.842642</td>\n",
       "      <td>-37284.718490</td>\n",
       "      <td>3657.426053</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.947068</td>\n",
       "      <td>0.133797</td>\n",
       "      <td>0.142573</td>\n",
       "      <td>0.008778</td>\n",
       "      <td>0.9</td>\n",
       "      <td>30</td>\n",
       "      <td>{'learning_decay': 0.9, 'n_components': 30}</td>\n",
       "      <td>-42668.175619</td>\n",
       "      <td>-44303.074287</td>\n",
       "      <td>-41380.703829</td>\n",
       "      <td>-34907.278409</td>\n",
       "      <td>-35309.299519</td>\n",
       "      <td>-39713.706333</td>\n",
       "      <td>3874.806225</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        5.845956      0.104144         0.153028        0.008257   \n",
       "1        4.790072      0.136663         0.127356        0.009912   \n",
       "2        4.786485      0.221351         0.133783        0.008054   \n",
       "3        5.026420      0.134606         0.138743        0.009025   \n",
       "4        4.966580      0.118991         0.137061        0.011771   \n",
       "5        4.834590      0.165225         0.136654        0.008059   \n",
       "6        6.020092      0.193916         0.153264        0.005375   \n",
       "7        5.220248      0.150028         0.149348        0.017156   \n",
       "8        5.390310      0.393962         0.143945        0.005179   \n",
       "9        5.142307      0.124830         0.137806        0.014752   \n",
       "10       4.848557      0.275192         0.134378        0.007419   \n",
       "11       4.737570      0.152812         0.133738        0.009334   \n",
       "12       5.814275      0.126414         0.152432        0.012295   \n",
       "13       5.283745      0.414544         0.144162        0.012471   \n",
       "14       4.894476      0.155976         0.136125        0.009681   \n",
       "15       4.836675      0.124257         0.132903        0.010458   \n",
       "16       4.834491      0.164795         0.133474        0.008701   \n",
       "17       4.947068      0.133797         0.142573        0.008778   \n",
       "\n",
       "   param_learning_decay param_n_components  \\\n",
       "0                   0.5                  5   \n",
       "1                   0.5                 10   \n",
       "2                   0.5                 15   \n",
       "3                   0.5                 20   \n",
       "4                   0.5                 25   \n",
       "5                   0.5                 30   \n",
       "6                   0.7                  5   \n",
       "7                   0.7                 10   \n",
       "8                   0.7                 15   \n",
       "9                   0.7                 20   \n",
       "10                  0.7                 25   \n",
       "11                  0.7                 30   \n",
       "12                  0.9                  5   \n",
       "13                  0.9                 10   \n",
       "14                  0.9                 15   \n",
       "15                  0.9                 20   \n",
       "16                  0.9                 25   \n",
       "17                  0.9                 30   \n",
       "\n",
       "                                         params  split0_test_score  \\\n",
       "0    {'learning_decay': 0.5, 'n_components': 5}      -28488.558606   \n",
       "1   {'learning_decay': 0.5, 'n_components': 10}      -33087.416985   \n",
       "2   {'learning_decay': 0.5, 'n_components': 15}      -35253.444208   \n",
       "3   {'learning_decay': 0.5, 'n_components': 20}      -38738.205443   \n",
       "4   {'learning_decay': 0.5, 'n_components': 25}      -40890.790108   \n",
       "5   {'learning_decay': 0.5, 'n_components': 30}      -42312.718053   \n",
       "6    {'learning_decay': 0.7, 'n_components': 5}      -28384.681008   \n",
       "7   {'learning_decay': 0.7, 'n_components': 10}      -32136.400816   \n",
       "8   {'learning_decay': 0.7, 'n_components': 15}      -35405.972626   \n",
       "9   {'learning_decay': 0.7, 'n_components': 20}      -37413.359244   \n",
       "10  {'learning_decay': 0.7, 'n_components': 25}      -41308.243278   \n",
       "11  {'learning_decay': 0.7, 'n_components': 30}      -42912.140151   \n",
       "12   {'learning_decay': 0.9, 'n_components': 5}      -28233.800754   \n",
       "13  {'learning_decay': 0.9, 'n_components': 10}      -32388.195499   \n",
       "14  {'learning_decay': 0.9, 'n_components': 15}      -34367.585513   \n",
       "15  {'learning_decay': 0.9, 'n_components': 20}      -38753.627273   \n",
       "16  {'learning_decay': 0.9, 'n_components': 25}      -40646.687393   \n",
       "17  {'learning_decay': 0.9, 'n_components': 30}      -42668.175619   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0       -28930.989002      -27153.294572      -22561.875711   \n",
       "1       -33513.216148      -30551.072069      -25346.915472   \n",
       "2       -35915.864000      -34697.528019      -28949.165100   \n",
       "3       -39143.454406      -37213.094375      -32353.431606   \n",
       "4       -42208.995511      -38992.560353      -33969.188965   \n",
       "5       -42835.708947      -40528.415772      -35837.246247   \n",
       "6       -28500.782516      -27141.818990      -22248.838749   \n",
       "7       -32668.045994      -31263.006276      -25633.120238   \n",
       "8       -36358.420192      -33618.448165      -29563.799087   \n",
       "9       -38599.003660      -37385.390037      -30739.987513   \n",
       "10      -41241.056900      -37776.620317      -33433.514468   \n",
       "11      -43850.598288      -40847.950884      -34917.495141   \n",
       "12      -28663.377834      -27096.922909      -22279.122570   \n",
       "13      -32939.353683      -31174.632342      -26850.588824   \n",
       "14      -37050.195673      -33863.430047      -29098.199969   \n",
       "15      -39044.787415      -36366.257228      -32174.173800   \n",
       "16      -40750.529572      -39331.553379      -32836.979462   \n",
       "17      -44303.074287      -41380.703829      -34907.278409   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0       -22773.777617    -25981.699101     2769.160296                3  \n",
       "1       -26303.785883    -29760.481311     3382.450596                5  \n",
       "2       -28021.033834    -32567.407032     3368.250559                8  \n",
       "3       -30823.541876    -35654.345541     3416.034759               12  \n",
       "4       -33496.305939    -37911.568175     3565.082989               15  \n",
       "5       -36164.727810    -39535.763366     2987.620678               16  \n",
       "6       -22571.661941    -25769.556641     2785.805419                2  \n",
       "7       -26415.223464    -29623.159358     2982.887194                4  \n",
       "8       -28930.848376    -32775.497689     3018.729329                9  \n",
       "9       -31671.982882    -35161.944667     3272.903597               10  \n",
       "10      -33856.014091    -37523.089811     3417.221520               14  \n",
       "11      -35919.661601    -39689.569213     3633.892372               17  \n",
       "12      -22358.029536    -25726.250721     2829.164949                1  \n",
       "13      -26296.863886    -29929.926847     2804.656683                6  \n",
       "14      -28309.757282    -32537.833697     3321.876528                7  \n",
       "15      -31393.346284    -35546.438400     3219.222014               11  \n",
       "16      -32857.842642    -37284.718490     3657.426053               13  \n",
       "17      -35309.299519    -39713.706333     3874.806225               18  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 1 words</th>\n",
       "      <th>Topic 1 weights</th>\n",
       "      <th>Topic 2 words</th>\n",
       "      <th>Topic 2 weights</th>\n",
       "      <th>Topic 3 words</th>\n",
       "      <th>Topic 3 weights</th>\n",
       "      <th>Topic 4 words</th>\n",
       "      <th>Topic 4 weights</th>\n",
       "      <th>Topic 5 words</th>\n",
       "      <th>Topic 5 weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>batteries</td>\n",
       "      <td>74.2</td>\n",
       "      <td>batteries</td>\n",
       "      <td>100.2</td>\n",
       "      <td>batteries</td>\n",
       "      <td>127.3</td>\n",
       "      <td>great</td>\n",
       "      <td>194.1</td>\n",
       "      <td>batteries</td>\n",
       "      <td>43.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>charge</td>\n",
       "      <td>40.9</td>\n",
       "      <td>charge</td>\n",
       "      <td>80.4</td>\n",
       "      <td>usb</td>\n",
       "      <td>80.4</td>\n",
       "      <td>good</td>\n",
       "      <td>146.9</td>\n",
       "      <td>lights</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>work</td>\n",
       "      <td>38.1</td>\n",
       "      <td>battery</td>\n",
       "      <td>50.9</td>\n",
       "      <td>good</td>\n",
       "      <td>72.8</td>\n",
       "      <td>works</td>\n",
       "      <td>101.5</td>\n",
       "      <td>good</td>\n",
       "      <td>24.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>money</td>\n",
       "      <td>35.1</td>\n",
       "      <td>use</td>\n",
       "      <td>41.9</td>\n",
       "      <td>rechargeable</td>\n",
       "      <td>67.6</td>\n",
       "      <td>product</td>\n",
       "      <td>89.2</td>\n",
       "      <td>worked</td>\n",
       "      <td>24.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buy</td>\n",
       "      <td>34.6</td>\n",
       "      <td>charged</td>\n",
       "      <td>40.4</td>\n",
       "      <td>battery</td>\n",
       "      <td>66.5</td>\n",
       "      <td>value</td>\n",
       "      <td>70.7</td>\n",
       "      <td>work</td>\n",
       "      <td>23.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>use</td>\n",
       "      <td>30.6</td>\n",
       "      <td>hold</td>\n",
       "      <td>40.2</td>\n",
       "      <td>charge</td>\n",
       "      <td>65.9</td>\n",
       "      <td>batteries</td>\n",
       "      <td>70.3</td>\n",
       "      <td>fine</td>\n",
       "      <td>23.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>long</td>\n",
       "      <td>26.1</td>\n",
       "      <td>long</td>\n",
       "      <td>38.7</td>\n",
       "      <td>charger</td>\n",
       "      <td>55.6</td>\n",
       "      <td>price</td>\n",
       "      <td>66.3</td>\n",
       "      <td>charger</td>\n",
       "      <td>23.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>don</td>\n",
       "      <td>25.7</td>\n",
       "      <td>camera</td>\n",
       "      <td>38.4</td>\n",
       "      <td>charging</td>\n",
       "      <td>52.3</td>\n",
       "      <td>long</td>\n",
       "      <td>53.9</td>\n",
       "      <td>battery</td>\n",
       "      <td>20.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>china</td>\n",
       "      <td>25.6</td>\n",
       "      <td>hours</td>\n",
       "      <td>34.2</td>\n",
       "      <td>like</td>\n",
       "      <td>49.1</td>\n",
       "      <td>work</td>\n",
       "      <td>49.4</td>\n",
       "      <td>charged</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>japan</td>\n",
       "      <td>22.6</td>\n",
       "      <td>don</td>\n",
       "      <td>31.0</td>\n",
       "      <td>great</td>\n",
       "      <td>48.6</td>\n",
       "      <td>far</td>\n",
       "      <td>43.6</td>\n",
       "      <td>charge</td>\n",
       "      <td>19.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>good</td>\n",
       "      <td>22.5</td>\n",
       "      <td>work</td>\n",
       "      <td>30.9</td>\n",
       "      <td>use</td>\n",
       "      <td>42.6</td>\n",
       "      <td>excellent</td>\n",
       "      <td>42.6</td>\n",
       "      <td>light</td>\n",
       "      <td>18.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>great</td>\n",
       "      <td>22.1</td>\n",
       "      <td>using</td>\n",
       "      <td>30.8</td>\n",
       "      <td>time</td>\n",
       "      <td>36.8</td>\n",
       "      <td>love</td>\n",
       "      <td>35.1</td>\n",
       "      <td>fit</td>\n",
       "      <td>18.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>recharge</td>\n",
       "      <td>22.1</td>\n",
       "      <td>months</td>\n",
       "      <td>29.3</td>\n",
       "      <td>nice</td>\n",
       "      <td>35.1</td>\n",
       "      <td>use</td>\n",
       "      <td>34.4</td>\n",
       "      <td>solar</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>battery</td>\n",
       "      <td>21.4</td>\n",
       "      <td>time</td>\n",
       "      <td>28.7</td>\n",
       "      <td>long</td>\n",
       "      <td>34.6</td>\n",
       "      <td>charge</td>\n",
       "      <td>34.2</td>\n",
       "      <td>thanks</td>\n",
       "      <td>17.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>remote</td>\n",
       "      <td>21.3</td>\n",
       "      <td>good</td>\n",
       "      <td>26.5</td>\n",
       "      <td>aaa</td>\n",
       "      <td>30.4</td>\n",
       "      <td>easy</td>\n",
       "      <td>33.5</td>\n",
       "      <td>charging</td>\n",
       "      <td>17.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bought</td>\n",
       "      <td>21.2</td>\n",
       "      <td>rechargeable</td>\n",
       "      <td>26.3</td>\n",
       "      <td>port</td>\n",
       "      <td>28.8</td>\n",
       "      <td>expected</td>\n",
       "      <td>32.5</td>\n",
       "      <td>used</td>\n",
       "      <td>17.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>amazon</td>\n",
       "      <td>21.0</td>\n",
       "      <td>dead</td>\n",
       "      <td>26.0</td>\n",
       "      <td>cable</td>\n",
       "      <td>27.2</td>\n",
       "      <td>perfect</td>\n",
       "      <td>29.9</td>\n",
       "      <td>does</td>\n",
       "      <td>16.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>time</td>\n",
       "      <td>20.4</td>\n",
       "      <td>life</td>\n",
       "      <td>25.3</td>\n",
       "      <td>plug</td>\n",
       "      <td>26.8</td>\n",
       "      <td>fast</td>\n",
       "      <td>29.9</td>\n",
       "      <td>just</td>\n",
       "      <td>16.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hold</td>\n",
       "      <td>19.7</td>\n",
       "      <td>just</td>\n",
       "      <td>25.2</td>\n",
       "      <td>best</td>\n",
       "      <td>25.9</td>\n",
       "      <td>money</td>\n",
       "      <td>29.0</td>\n",
       "      <td>led</td>\n",
       "      <td>13.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>used</td>\n",
       "      <td>19.7</td>\n",
       "      <td>power</td>\n",
       "      <td>23.6</td>\n",
       "      <td>easy</td>\n",
       "      <td>25.5</td>\n",
       "      <td>battery</td>\n",
       "      <td>29.0</td>\n",
       "      <td>great</td>\n",
       "      <td>13.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic 1 words Topic 1 weights Topic 2 words Topic 2 weights Topic 3 words  \\\n",
       "0      batteries            74.2     batteries           100.2     batteries   \n",
       "1         charge            40.9        charge            80.4           usb   \n",
       "2           work            38.1       battery            50.9          good   \n",
       "3          money            35.1           use            41.9  rechargeable   \n",
       "4            buy            34.6       charged            40.4       battery   \n",
       "5            use            30.6          hold            40.2        charge   \n",
       "6           long            26.1          long            38.7       charger   \n",
       "7            don            25.7        camera            38.4      charging   \n",
       "8          china            25.6         hours            34.2          like   \n",
       "9          japan            22.6           don            31.0         great   \n",
       "10          good            22.5          work            30.9           use   \n",
       "11         great            22.1         using            30.8          time   \n",
       "12      recharge            22.1        months            29.3          nice   \n",
       "13       battery            21.4          time            28.7          long   \n",
       "14        remote            21.3          good            26.5           aaa   \n",
       "15        bought            21.2  rechargeable            26.3          port   \n",
       "16        amazon            21.0          dead            26.0         cable   \n",
       "17          time            20.4          life            25.3          plug   \n",
       "18          hold            19.7          just            25.2          best   \n",
       "19          used            19.7         power            23.6          easy   \n",
       "\n",
       "   Topic 3 weights Topic 4 words Topic 4 weights Topic 5 words Topic 5 weights  \n",
       "0            127.3         great           194.1     batteries            43.3  \n",
       "1             80.4          good           146.9        lights            26.0  \n",
       "2             72.8         works           101.5          good            24.9  \n",
       "3             67.6       product            89.2        worked            24.6  \n",
       "4             66.5         value            70.7          work            23.8  \n",
       "5             65.9     batteries            70.3          fine            23.6  \n",
       "6             55.6         price            66.3       charger            23.4  \n",
       "7             52.3          long            53.9       battery            20.9  \n",
       "8             49.1          work            49.4       charged            20.0  \n",
       "9             48.6           far            43.6        charge            19.2  \n",
       "10            42.6     excellent            42.6         light            18.6  \n",
       "11            36.8          love            35.1           fit            18.6  \n",
       "12            35.1           use            34.4         solar            18.5  \n",
       "13            34.6        charge            34.2        thanks            17.7  \n",
       "14            30.4          easy            33.5      charging            17.4  \n",
       "15            28.8      expected            32.5          used            17.1  \n",
       "16            27.2       perfect            29.9          does            16.9  \n",
       "17            26.8          fast            29.9          just            16.8  \n",
       "18            25.9         money            29.0           led            13.6  \n",
       "19            25.5       battery            29.0         great            13.5  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a function to inspect the topics we created \n",
    "def display_topics(model, feature_names, n_top_words):\n",
    "    '''\n",
    "    INPUTS:\n",
    "        model - the model we created\n",
    "        feature_names - tells us what word each column in the matric represents\n",
    "        n_top_words - number of top words to display\n",
    "\n",
    "    OUTPUTS:\n",
    "        a dataframe that contains the topics we created and the weights of each token\n",
    "    '''\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx+1)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx+1)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)\n",
    "\n",
    "\n",
    "display_topics(best_lda_model, tfidf_vectorizer.get_feature_names(), n_top_words = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "cleaned['processed_reviews'] = cleaned['title_desc'].apply(lambda x: re.sub('[,\\.!?]', '', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned['processed_reviews'] = cleaned['processed_reviews'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviews_to_words(reviews):\n",
    "    review_list = reviews.values.tolist()\n",
    "    for review in reviews:\n",
    "        yield(simple_preprocess(review, deacc=True)) #deacc=True removes punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(reviews_to_words(cleaned['processed_reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = gensim.models.Phrases(tokens, min_count=2)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ryankirkland/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['good', 'battery', 'batteries', 'use', 'get', 'great', 'buy'])\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Define functions for stopwords, lemmatization, bigrams, and trigrams\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return[[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(tokens)\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 2), (7, 1), (8, 4), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 5), (15, 2), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 2), (53, 2), (54, 1), (55, 1), (56, 1), (57, 3), (58, 1), (59, 2), (60, 1), (61, 9), (62, 1), (63, 1)]]\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=5, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.025*\"charge\" + 0.016*\"long\" + 0.013*\"life\" + 0.007*\"capacity\" + '\n",
      "  '0.007*\"seem\" + 0.006*\"recharge\" + 0.006*\"hold_charge\" + 0.006*\"volt\" + '\n",
      "  '0.006*\"issue\" + 0.006*\"well\"'),\n",
      " (1,\n",
      "  '0.054*\"work\" + 0.030*\"product\" + 0.019*\"price\" + 0.019*\"rechargeable\" + '\n",
      "  '0.017*\"far\" + 0.017*\"quality\" + 0.014*\"make\" + 0.012*\"expect\" + '\n",
      "  '0.011*\"charge\" + 0.011*\"well\"'),\n",
      " (2,\n",
      "  '0.036*\"charge\" + 0.020*\"use\" + 0.019*\"value\" + 0.016*\"time\" + '\n",
      "  '0.014*\"rechargeable\" + 0.013*\"far\" + 0.013*\"charger\" + 0.012*\"recharge\" + '\n",
      "  '0.012*\"usb\" + 0.011*\"work\"'),\n",
      " (3,\n",
      "  '0.020*\"charge\" + 0.017*\"charger\" + 0.016*\"use\" + 0.015*\"work\" + '\n",
      "  '0.014*\"rechargeable\" + 0.012*\"power\" + 0.010*\"light\" + 0.010*\"capacity\" + '\n",
      "  '0.009*\"last\" + 0.007*\"day\"'),\n",
      " (4,\n",
      "  '0.058*\"charge\" + 0.017*\"use\" + 0.016*\"last\" + 0.016*\"rechargeable\" + '\n",
      "  '0.013*\"time\" + 0.013*\"charger\" + 0.011*\"work\" + 0.009*\"need\" + 0.009*\"buy\" '\n",
      "  '+ 0.008*\"long\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.44626446266732095\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b,\n",
    "                                           per_word_topics=True)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
    "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
    "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.75), \n",
    "               corpus]\n",
    "corpus_title = ['100% Corpus']\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/540 [00:47<1:10:06,  7.88s/it]\n",
      " 50%|█████     | 270/540 [34:28<34:28,  7.66s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=540)\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
    "                                                  k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa66b7593c8>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2)\n",
    "axs[0].plot(mr_alpha_group['Alpha'], mr_alpha_group['Coherence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = pd.DataFrame(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr['Alpha'] = mr['Alpha'].map({'symmetric': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.31, 0.61, 0.9099999999999999, 'symmetric', 'asymmetric'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr['Alpha'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_alpha = mr[(mr['Alpha'] != 'symmetric') & (mr['Alpha'] != 'asymmetric')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_alpha_group = mr_alpha.groupby('Alpha').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Validation_Set</th>\n",
       "      <th>Topics</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Coherence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>100% Corpus</td>\n",
       "      <td>3</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.552422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>100% Corpus</td>\n",
       "      <td>7</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.547111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>100% Corpus</td>\n",
       "      <td>8</td>\n",
       "      <td>symmetric</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.542077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>100% Corpus</td>\n",
       "      <td>8</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.537118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>100% Corpus</td>\n",
       "      <td>9</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.535743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>100% Corpus</td>\n",
       "      <td>7</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.350577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>100% Corpus</td>\n",
       "      <td>3</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.349813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>100% Corpus</td>\n",
       "      <td>10</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.348432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100% Corpus</td>\n",
       "      <td>2</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.344646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>100% Corpus</td>\n",
       "      <td>8</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.335796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Validation_Set  Topics      Alpha  Beta  Coherence\n",
       "38     100% Corpus       3       0.31  0.91   0.552422\n",
       "158    100% Corpus       7       0.31  0.91   0.547111\n",
       "203    100% Corpus       8  symmetric  0.91   0.542077\n",
       "188    100% Corpus       8       0.31  0.91   0.537118\n",
       "218    100% Corpus       9       0.31  0.91   0.535743\n",
       "..             ...     ...        ...   ...        ...\n",
       "168    100% Corpus       7       0.91  0.91   0.350577\n",
       "48     100% Corpus       3       0.91  0.91   0.349813\n",
       "258    100% Corpus      10       0.91  0.91   0.348432\n",
       "18     100% Corpus       2       0.91  0.91   0.344646\n",
       "198    100% Corpus       8       0.91  0.91   0.335796\n",
       "\n",
       "[270 rows x 5 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr.sort_values('Coherence', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2378803366954205"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.552422/0.44626446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=3, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=0.31,\n",
    "                                           eta=0.91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el156951403526943748323642794803\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el156951403526943748323642794803_data = {\"mdsDat\": {\"x\": [-0.08135707446468236, 0.03275940300393203, 0.04859767146075035], \"y\": [0.008838998609851326, -0.07252496203800438, 0.06368596342815305], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [61.804134368896484, 24.936058044433594, 13.259803771972656]}, \"tinfo\": {\"Term\": [\"value\", \"work\", \"charge\", \"last\", \"long\", \"product\", \"life\", \"price\", \"hour\", \"light\", \"money\", \"quality\", \"device\", \"voltage\", \"capacity\", \"day\", \"make\", \"advertise\", \"cell\", \"brand\", \"seem\", \"night\", \"expect\", \"worth\", \"well\", \"minute\", \"rechargeable\", \"hold_charge\", \"test\", \"nimh\", \"remote\", \"phone\", \"last_long\", \"wait\", \"plug\", \"kid\", \"house\", \"wish\", \"amazing\", \"strong\", \"still_goe\", \"solar\", \"already\", \"love\", \"yet\", \"red\", \"usb_port\", \"pair\", \"frequently\", \"cool\", \"save_money\", \"constantly\", \"highly_recommend\", \"game_controller\", \"disappointed\", \"gaming\", \"hard\", \"tired\", \"buying\", \"rotate\", \"easy\", \"charger\", \"usb\", \"mouse\", \"recharge\", \"time\", \"charge\", \"week\", \"controller\", \"use\", \"set\", \"buy\", \"replace\", \"month\", \"get\", \"really\", \"come\", \"year\", \"lot\", \"last\", \"go\", \"far\", \"rechargeable\", \"need\", \"thing\", \"try\", \"take\", \"purchase\", \"put\", \"work\", \"well\", \"long\", \"light\", \"seem\", \"hour\", \"make\", \"advertise\", \"voltage\", \"describe\", \"claim\", \"service\", \"measure\", \"discharge_rate\", \"current\", \"shelf_life\", \"ship\", \"range\", \"cell\", \"voltage_drop\", \"amp\", \"load\", \"circuitry\", \"motor\", \"seller\", \"fast_shipping\", \"fast_shippe\", \"temperature\", \"minimum\", \"high_drain\", \"output_voltage\", \"drop\", \"word\", \"perhaps\", \"country\", \"self_discharge\", \"application\", \"appear\", \"likely\", \"deliver\", \"thank\", \"exactly\", \"output\", \"volt\", \"capacity\", \"quality\", \"discharge\", \"nimh\", \"five_star\", \"test\", \"state\", \"product\", \"device\", \"work\", \"low\", \"expect\", \"tenavolt\", \"brand\", \"make\", \"high\", \"camera\", \"rechargeable\", \"issue\", \"power\", \"well\", \"price\", \"need\", \"charge\", \"far\", \"use\", \"go\", \"seem\", \"purchase\", \"long\", \"charger\", \"headlamp\", \"night\", \"worthless\", \"failure_rate\", \"solar_light\", \"dont_hold\", \"horrible\", \"terrible\", \"morning\", \"high_hope\", \"holiday\", \"holds_charge\", \"fairy_light\", \"candle\", \"overheat\", \"barely\", \"okay\", \"value\", \"player\", \"accept_charge\", \"exchange\", \"else\", \"garbage\", \"walkie_talkie\", \"melt\", \"three_time\", \"reputation\", \"definately\", \"gave_star\", \"arrive_fully\", \"job\", \"suck\", \"usage\", \"dim\", \"average\", \"minute\", \"expectation\", \"bright\", \"basic\", \"price\", \"second\", \"life\", \"durable\", \"worth\", \"long\", \"money\", \"hour\", \"light\", \"last\", \"charge\", \"disappoint\", \"brand\", \"product\", \"seem\", \"day\", \"eneloop\", \"hold_charge\", \"well\", \"quality\", \"use\", \"first\", \"time\", \"buy\", \"flashlight\", \"work\", \"get\", \"come\", \"take\", \"try\", \"recharge\"], \"Freq\": [240.0, 1130.0, 2591.0, 716.0, 456.0, 489.0, 273.0, 297.0, 315.0, 334.0, 254.0, 215.0, 351.0, 126.0, 206.0, 244.0, 399.0, 89.0, 113.0, 149.0, 364.0, 58.0, 172.0, 103.0, 455.0, 74.0, 1003.0, 156.0, 133.0, 95.0, 148.44883728027344, 78.58063507080078, 97.97041320800781, 51.03410720825195, 171.76451110839844, 48.18570327758789, 47.574012756347656, 61.766883850097656, 39.61997985839844, 55.70902633666992, 30.798274993896484, 38.75174331665039, 65.65726470947266, 263.2027893066406, 86.01679992675781, 62.20570755004883, 53.753326416015625, 45.084110260009766, 23.393014907836914, 48.1385612487793, 45.009525299072266, 32.050933837890625, 31.6107234954834, 23.114967346191406, 42.29676818847656, 21.318845748901367, 40.892066955566406, 17.570127487182617, 46.323219299316406, 15.927756309509277, 218.20086669921875, 720.0515747070312, 298.439453125, 85.53954315185547, 461.87939453125, 598.5242309570312, 2151.662353515625, 174.10914611816406, 161.474609375, 829.3602294921875, 198.3640899658203, 443.2888488769531, 170.23577880859375, 231.6878662109375, 314.1014099121094, 212.53515625, 274.417236328125, 206.2547149658203, 139.1930694580078, 546.2086181640625, 342.78643798828125, 352.23492431640625, 715.8422241210938, 359.6468505859375, 143.97369384765625, 231.64312744140625, 236.34718322753906, 250.56588745117188, 192.5117950439453, 601.463623046875, 285.0536804199219, 273.7546691894531, 227.8644256591797, 235.17916870117188, 215.1911163330078, 226.95326232910156, 86.90711212158203, 122.3984375, 47.55747604370117, 32.15917205810547, 28.674339294433594, 19.087596893310547, 13.19668960571289, 38.190589904785156, 11.71349048614502, 12.901570320129395, 12.149054527282715, 98.45516204833984, 10.003778457641602, 13.095087051391602, 24.454275131225586, 10.431388854980469, 15.248984336853027, 29.578166961669922, 8.710192680358887, 8.158320426940918, 9.452722549438477, 11.4093599319458, 13.791230201721191, 7.738743305206299, 35.25223159790039, 10.462752342224121, 8.112634658813477, 8.478785514831543, 8.5316743850708, 37.112464904785156, 38.4713020324707, 13.131232261657715, 25.994098663330078, 44.55640411376953, 43.836936950683594, 31.316011428833008, 74.5776596069336, 149.1348114013672, 154.28541564941406, 63.62375259399414, 69.4754409790039, 19.936429977416992, 86.56644439697266, 29.86532211303711, 254.12130737304688, 188.53768920898438, 482.7641296386719, 79.7551040649414, 97.91822052001953, 22.46966552734375, 79.78610229492188, 164.14459228515625, 58.26376724243164, 62.97748565673828, 259.62286376953125, 69.71131134033203, 90.40202331542969, 112.36119079589844, 87.6414566040039, 101.26109313964844, 192.89976501464844, 89.74625396728516, 102.04251098632812, 68.51496887207031, 64.47151184082031, 57.82778549194336, 59.26901626586914, 59.48460388183594, 18.31399917602539, 47.001983642578125, 11.889552116394043, 7.014488220214844, 16.51148796081543, 6.144005298614502, 9.430070877075195, 17.005475997924805, 10.37964916229248, 5.416112899780273, 4.967249393463135, 8.09016227722168, 5.596344470977783, 21.91851806640625, 5.965686321258545, 14.355056762695312, 4.787087440490723, 172.63902282714844, 5.0658183097839355, 4.957405090332031, 6.251253604888916, 5.200800895690918, 18.386056900024414, 6.344030857086182, 5.038093090057373, 4.3475751876831055, 4.352495193481445, 3.872699737548828, 4.612543106079102, 3.7108960151672363, 20.8823299407959, 9.783851623535156, 14.558536529541016, 9.666132926940918, 12.829176902770996, 39.3005256652832, 12.473121643066406, 18.693954467773438, 7.758337497711182, 103.23521423339844, 27.145986557006836, 95.46260833740234, 16.156383514404297, 41.660400390625, 123.74417877197266, 72.95990753173828, 85.14830780029297, 86.14741516113281, 134.50482177734375, 246.83799743652344, 19.72014808654785, 40.68876647949219, 76.8869400024414, 64.70330047607422, 51.03506088256836, 24.935213088989258, 36.83639144897461, 58.07526779174805, 38.86157989501953, 60.97159194946289, 32.37215042114258, 49.03732681274414, 43.73220443725586, 30.3555965423584, 46.76312255859375, 33.52238464355469, 31.83124351501465, 31.030366897583008, 30.090669631958008, 29.604766845703125], \"Total\": [240.0, 1130.0, 2591.0, 716.0, 456.0, 489.0, 273.0, 297.0, 315.0, 334.0, 254.0, 215.0, 351.0, 126.0, 206.0, 244.0, 399.0, 89.0, 113.0, 149.0, 364.0, 58.0, 172.0, 103.0, 455.0, 74.0, 1003.0, 156.0, 133.0, 95.0, 151.23802185058594, 81.12657928466797, 101.1764907836914, 53.0335807800293, 178.55007934570312, 50.09233474731445, 49.519100189208984, 64.64244079589844, 41.548709869384766, 58.535377502441406, 32.395809173583984, 40.79783630371094, 69.1596908569336, 277.3158874511719, 91.27024841308594, 66.04144287109375, 57.08502197265625, 47.97405242919922, 24.934907913208008, 51.31188201904297, 47.99280548095703, 34.26498794555664, 33.833091735839844, 24.77721405029297, 45.37939453125, 22.890771865844727, 44.03569793701172, 18.9222412109375, 49.91902542114258, 17.25118637084961, 236.3434295654297, 794.3987426757812, 326.3800354003906, 93.20867919921875, 523.8199462890625, 684.3069458007812, 2591.39990234375, 194.26702880859375, 180.2037353515625, 992.3743286132812, 223.64556884765625, 517.6028442382812, 192.63223266601562, 272.2049865722656, 380.2559814453125, 250.12387084960938, 332.3371276855469, 244.76568603515625, 159.26739501953125, 716.2762451171875, 432.66815185546875, 447.9129638671875, 1003.1906127929688, 466.0520935058594, 168.55984497070312, 291.37347412109375, 300.2126770019531, 335.5429382324219, 247.1552734375, 1130.990966796875, 455.4901123046875, 456.7678527832031, 334.7327880859375, 364.35400390625, 315.59429931640625, 399.5653991699219, 89.24736785888672, 126.76270294189453, 49.30742645263672, 34.58954620361328, 30.92969512939453, 21.049861907958984, 14.914033889770508, 43.248661041259766, 13.447192192077637, 14.845529556274414, 14.003507614135742, 113.72930145263672, 11.570947647094727, 15.199180603027344, 28.43400764465332, 12.198627471923828, 17.852079391479492, 34.856449127197266, 10.276103019714355, 9.65158748626709, 11.198721885681152, 13.550254821777344, 16.381803512573242, 9.311881065368652, 42.59013366699219, 12.659290313720703, 9.85212230682373, 10.301484107971191, 10.40999984741211, 45.324745178222656, 47.05876541137695, 16.04349136352539, 32.25263595581055, 56.052616119384766, 55.770240783691406, 39.34724807739258, 98.08285522460938, 206.5185546875, 215.16189575195312, 84.1998291015625, 95.32921600341797, 24.872966766357422, 133.0800018310547, 39.66042709350586, 489.9642639160156, 351.0928955078125, 1130.990966796875, 133.81727600097656, 172.5054168701172, 28.812786102294922, 149.24842834472656, 399.5653991699219, 100.15061950683594, 117.83662414550781, 1003.1906127929688, 148.3045196533203, 285.4682312011719, 455.4901123046875, 297.81414794921875, 466.0520935058594, 2591.39990234375, 447.9129638671875, 992.3743286132812, 432.66815185546875, 364.35400390625, 335.5429382324219, 456.7678527832031, 794.3987426757812, 21.63459014892578, 58.66973876953125, 14.874330520629883, 8.794557571411133, 20.735118865966797, 7.8119096755981445, 12.159083366394043, 22.198795318603516, 13.660005569458008, 7.28489351272583, 6.735475540161133, 10.988130569458008, 7.603755950927734, 29.86109733581543, 8.158565521240234, 19.63410758972168, 6.632447719573975, 240.23980712890625, 7.051936149597168, 6.92551851272583, 8.740211486816406, 7.348745346069336, 26.005462646484375, 9.010272979736328, 7.160421848297119, 6.2101335525512695, 6.22003173828125, 5.573631763458252, 6.6764373779296875, 5.392996311187744, 30.6366024017334, 14.695929527282715, 22.516340255737305, 15.031785011291504, 20.597702026367188, 74.79115295410156, 20.545570373535156, 34.50569152832031, 11.880254745483398, 297.81414794921875, 56.36359786987305, 273.5609130859375, 29.906715393066406, 103.33821868896484, 456.7678527832031, 254.02859497070312, 315.59429931640625, 334.7327880859375, 716.2762451171875, 2591.39990234375, 45.938087463378906, 149.24842834472656, 489.9642639160156, 364.35400390625, 244.90887451171875, 73.32482147216797, 156.42567443847656, 455.4901123046875, 215.16189575195312, 992.3743286132812, 191.26278686523438, 684.3069458007812, 517.6028442382812, 173.07522583007812, 1130.990966796875, 380.2559814453125, 332.3371276855469, 300.2126770019531, 291.37347412109375, 523.8199462890625], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.715099811553955, -6.351200103759766, -6.13070011138916, -6.782800197601318, -5.569200038909912, -6.8403000831604, -6.853000164031982, -6.5920000076293945, -7.035999774932861, -6.695199966430664, -7.287899971008301, -7.058199882507324, -6.530900001525879, -5.142399787902832, -6.260799884796143, -6.58489990234375, -6.730899810791016, -6.906799793243408, -7.562900066375732, -6.84119987487793, -6.9085001945495605, -7.248000144958496, -7.2617998123168945, -7.57480001449585, -6.970600128173828, -7.655700206756592, -7.00439977645874, -7.849100112915039, -6.879700183868408, -7.947299957275391, -5.329899787902832, -4.136000156402588, -5.0167999267578125, -6.26639986038208, -4.579999923706055, -4.320899963378906, -3.041300058364868, -5.555600166320801, -5.63100004196167, -3.9946999549865723, -5.42519998550415, -4.621099948883057, -5.578100204467773, -5.269899845123291, -4.96560001373291, -5.356200218200684, -5.1006999015808105, -5.386199951171875, -5.7795000076293945, -4.412300109863281, -4.878200054168701, -4.85099983215332, -4.141900062561035, -4.8302001953125, -5.745699882507324, -5.270100116729736, -5.25, -5.1915998458862305, -5.4552001953125, -4.315999984741211, -5.062699794769287, -5.103099822998047, -5.286600112915039, -5.255000114440918, -5.343800067901611, -5.290599822998047, -5.342800140380859, -5.000400066375732, -5.945700168609619, -6.336999893188477, -6.451700210571289, -6.85860013961792, -7.227700233459473, -6.16510009765625, -7.34689998626709, -7.25029993057251, -7.310400009155273, -5.218100070953369, -7.504700183868408, -7.235400199890137, -6.610899925231934, -7.462900161743164, -7.083199977874756, -6.420599937438965, -7.643199920654297, -7.708600044250488, -7.561399936676025, -7.373199939727783, -7.183599948883057, -7.76140022277832, -6.245100021362305, -7.45989990234375, -7.714300155639648, -7.670100212097168, -7.663899898529053, -6.193699836730957, -6.157800197601318, -7.232699871063232, -6.549799919128418, -6.010900020599365, -6.027200222015381, -6.363500118255615, -5.495800018310547, -4.802800178527832, -4.768899917602539, -5.654699802398682, -5.566699981689453, -6.815100193023682, -5.346799850463867, -6.410999774932861, -4.269899845123291, -4.568399906158447, -3.6280999183654785, -5.428699970245361, -5.223499774932861, -6.695499897003174, -5.428299903869629, -4.706900119781494, -5.742700099945068, -5.664899826049805, -4.2484002113342285, -5.563300132751465, -5.303400039672852, -5.085999965667725, -5.334400177001953, -5.190000057220459, -4.545499801635742, -5.310699939727783, -5.182300090789795, -5.580599784851074, -5.64139986038208, -5.750199794769287, -5.725599765777588, -5.7220001220703125, -6.268400192260742, -5.325900077819824, -6.700399875640869, -7.228099822998047, -6.372000217437744, -7.360599994659424, -6.932199954986572, -6.342599868774414, -6.83620023727417, -7.486700057983398, -7.573200225830078, -7.085400104522705, -7.453999996185303, -6.088799953460693, -7.390100002288818, -6.51200008392334, -7.610199928283691, -4.024899959564209, -7.553599834442139, -7.575200080871582, -7.343299865722656, -7.527299880981445, -6.264500141143799, -7.32859992980957, -7.559100151062012, -7.706500053405762, -7.7052998542785645, -7.8221001625061035, -7.647299766540527, -7.864799976348877, -6.137199878692627, -6.895400047302246, -6.497900009155273, -6.90749979019165, -6.6244001388549805, -5.504899978637695, -6.652500152587891, -6.247900009155273, -7.127299785614014, -4.539100170135498, -5.874899864196777, -4.617400169372559, -6.393799781799316, -5.446499824523926, -4.357900142669678, -4.886199951171875, -4.7316999435424805, -4.71999979019165, -4.274499893188477, -3.6673998832702637, -6.194499969482422, -5.470099925994873, -4.833799839019775, -5.00629997253418, -5.243599891662598, -5.959799766540527, -5.5696001052856445, -5.1143999099731445, -5.51609992980957, -5.065700054168701, -5.698800086975098, -5.2835001945495605, -5.3979997634887695, -5.7631001472473145, -5.330999851226807, -5.663899898529053, -5.71560001373291, -5.741099834442139, -5.771900177001953, -5.7881999015808105], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.4625999927520752, 0.44929999113082886, 0.4490000009536743, 0.44279998540878296, 0.4424999952316284, 0.4424000084400177, 0.44110000133514404, 0.435699999332428, 0.43369999527931213, 0.4316999912261963, 0.43059998750686646, 0.42969998717308044, 0.4291999936103821, 0.42899999022483826, 0.4219000041484833, 0.4214000105857849, 0.421099990606308, 0.41909998655319214, 0.4174000024795532, 0.4174000024795532, 0.4169999957084656, 0.41440001130104065, 0.4133000075817108, 0.41179999709129333, 0.4108999967575073, 0.4101000130176544, 0.40709999203681946, 0.40709999203681946, 0.40639999508857727, 0.40139999985694885, 0.40130001306533813, 0.3828999996185303, 0.39169999957084656, 0.3953000009059906, 0.3553999960422516, 0.3472999930381775, 0.295199990272522, 0.3716000020503998, 0.3714999854564667, 0.301800012588501, 0.3612000048160553, 0.326200008392334, 0.35760000348091125, 0.3199999928474426, 0.29010000824928284, 0.31839999556541443, 0.2897000014781952, 0.3100000023841858, 0.3465000092983246, 0.210099995136261, 0.2483000010251999, 0.24089999496936798, 0.1437000036239624, 0.22200000286102295, 0.32350000739097595, 0.251800000667572, 0.24199999868869781, 0.1891999989748001, 0.2312999963760376, -0.15029999613761902, 0.012500000186264515, -0.030700000002980232, 0.0966000035405159, 0.04340000078082085, 0.09830000251531601, -0.0843999981880188, 1.3623000383377075, 1.3538000583648682, 1.3526999950408936, 1.315999984741211, 1.313099980354309, 1.2910000085830688, 1.2664999961853027, 1.2645000219345093, 1.2508000135421753, 1.2484999895095825, 1.2467999458312988, 1.2446000576019287, 1.243299961090088, 1.23989999294281, 1.2381000518798828, 1.2323999404907227, 1.2311999797821045, 1.2246999740600586, 1.2235000133514404, 1.2208000421524048, 1.2194000482559204, 1.2168999910354614, 1.2166999578475952, 1.2037999629974365, 1.1998000144958496, 1.1983000040054321, 1.194599986076355, 1.194100022315979, 1.1899000406265259, 1.1890000104904175, 1.187399983406067, 1.1885000467300415, 1.1730999946594238, 1.1592999696731567, 1.1481000185012817, 1.160599946975708, 1.11489999294281, 1.0633000135421753, 1.0563000440597534, 1.1086000204086304, 1.0724999904632568, 1.1676000356674194, 0.9588000178337097, 1.1052000522613525, 0.7322999835014343, 0.7670999765396118, 0.5375000238418579, 0.8712999820709229, 0.8226000070571899, 1.1402000188827515, 0.7626000046730042, 0.4991999864578247, 0.8471999764442444, 0.7623000144958496, 0.03709999844431877, 0.6340000033378601, 0.23899999260902405, -0.01080000028014183, 0.1656000018119812, -0.13770000636577606, -1.208899974822998, -0.21879999339580536, -0.8859000205993652, -0.45410001277923584, -0.34299999475479126, -0.3693999946117401, -0.6531999707221985, -1.2029999494552612, 1.8538000583648682, 1.798699975013733, 1.7964999675750732, 1.7942999601364136, 1.7927000522613525, 1.7803000211715698, 1.7662999629974365, 1.7539000511169434, 1.7458000183105469, 1.7239999771118164, 1.71589994430542, 1.7143000364303589, 1.7138999700546265, 1.7111999988555908, 1.7073999643325806, 1.7072999477386475, 1.6943999528884888, 1.690000057220459, 1.6895999908447266, 1.6861000061035156, 1.6852999925613403, 1.6747000217437744, 1.673699975013733, 1.669600009918213, 1.6689000129699707, 1.6639000177383423, 1.6634000539779663, 1.6562999486923218, 1.6505999565124512, 1.6466000080108643, 1.6370999813079834, 1.6136000156402588, 1.5844000577926636, 1.5788999795913696, 1.5470000505447388, 1.3769999742507935, 1.521399974822998, 1.4075000286102295, 1.5943000316619873, 0.9610000252723694, 1.2898000478744507, 0.9675999879837036, 1.4047000408172607, 1.1119999885559082, 0.7145000100135803, 0.7728999853134155, 0.7103999853134155, 0.6632000207901001, 0.3479999899864197, -0.33079999685287476, 1.1748000383377075, 0.72079998254776, 0.16840000450611115, 0.2921000123023987, 0.45210000872612, 0.9417999982833862, 0.5742999911308289, -0.03920000046491623, 0.3089999854564667, -0.7692999839782715, 0.24410000443458557, -0.6154000163078308, -0.4507000148296356, 0.27970001101493835, -1.1653000116348267, -0.4081999957561493, -0.325300008058548, -0.249099999666214, -0.25, -0.8528000116348267]}, \"token.table\": {\"Topic\": [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3], \"Freq\": [0.14439351856708527, 0.14439351856708527, 0.7219675779342651, 0.011204812675714493, 0.9748186469078064, 0.011204812675714493, 0.9543130993843079, 0.02891857922077179, 0.014459289610385895, 0.9627254605293274, 0.024068135768175125, 0.024068135768175125, 0.06579302251338959, 0.855309247970581, 0.06579302251338959, 0.1487501859664917, 0.8075010180473328, 0.042500052601099014, 0.15444102883338928, 0.816331148147583, 0.022063003852963448, 0.18542568385601044, 0.18542568385601044, 0.7417027354240417, 0.194196417927742, 0.194196417927742, 0.6311383843421936, 0.15279532968997955, 0.15279532968997955, 0.7130448818206787, 0.1683465540409088, 0.1683465540409088, 0.6733862161636353, 0.19430691003799438, 0.5360190272331238, 0.2747097611427307, 0.2898072600364685, 0.17388436198234558, 0.5506337881088257, 0.8558685779571533, 0.0598914809525013, 0.08500725775957108, 0.921492338180542, 0.06009732559323311, 0.02003244310617447, 0.44128894805908203, 0.534638524055481, 0.02545897848904133, 0.23441872000694275, 0.03348838910460472, 0.7367445230484009, 0.19852937757968903, 0.7214847803115845, 0.07747487723827362, 0.09672089666128159, 0.8616952896118164, 0.03517123684287071, 0.8304391503334045, 0.07447712123394012, 0.09531527757644653, 0.9063458442687988, 0.07427000999450684, 0.018882205709815025, 0.08197643607854843, 0.8197643756866455, 0.08197643607854843, 0.057820938527584076, 0.9251350164413452, 0.028910469263792038, 0.8244640231132507, 0.07823380827903748, 0.09628776460886002, 0.9338979125022888, 0.05836861953139305, 0.029184309765696526, 0.8934332132339478, 0.0166478231549263, 0.08878839015960693, 0.935455858707428, 0.038977328687906265, 0.019488664343953133, 0.09707339107990265, 0.7765871286392212, 0.09707339107990265, 0.09248841553926468, 0.8786399364471436, 0.02312210388481617, 0.7390503883361816, 0.0530809685587883, 0.20824071764945984, 0.179416224360466, 0.179416224360466, 0.717664897441864, 0.12402087450027466, 0.8061356544494629, 0.06201043725013733, 0.02028091996908188, 0.9734841585159302, 0.02028091996908188, 0.44432684779167175, 0.5383190512657166, 0.01993774250149727, 0.26610279083251953, 0.13305139541625977, 0.6652569770812988, 0.4353685677051544, 0.15237900614738464, 0.4353685677051544, 0.925530195236206, 0.044072866439819336, 0.022036433219909668, 0.19002413749694824, 0.760096549987793, 0.04750603437423706, 0.06705094128847122, 0.8716621994972229, 0.06705094128847122, 0.12800967693328857, 0.12800967693328857, 0.7680580615997314, 0.11739808320999146, 0.8217865824699402, 0.04695923253893852, 0.13374923169612885, 0.3343730568885803, 0.5349969267845154, 0.9223865270614624, 0.05077357217669487, 0.025386786088347435, 0.1360776573419571, 0.1360776573419571, 0.6803882718086243, 0.1500174105167389, 0.5046040415763855, 0.3409486711025238, 0.1972378045320511, 0.7889512181282043, 0.017930708825588226, 0.22882741689682007, 0.11441370844841003, 0.6864822506904602, 0.41158127784729004, 0.5680981278419495, 0.023187678307294846, 0.09734458476305008, 0.29203376173973083, 0.5840675234794617, 0.11370668560266495, 0.11370668560266495, 0.7959467768669128, 0.13151395320892334, 0.13151395320892334, 0.78908371925354, 0.7858669757843018, 0.20093189179897308, 0.0133954593911767, 0.10360989719629288, 0.828879177570343, 0.10360989719629288, 0.09731315076351166, 0.8758183717727661, 0.09731315076351166, 0.6744647026062012, 0.15685226023197174, 0.16730907559394836, 0.16081716120243073, 0.8040858507156372, 0.04020429030060768, 0.5835613012313843, 0.24266904592514038, 0.1733350306749344, 0.9224016666412354, 0.040104418992996216, 0.040104418992996216, 0.9282722473144531, 0.04035966098308563, 0.04035966098308563, 0.9174002408981323, 0.04368572682142258, 0.04368572682142258, 0.1922672986984253, 0.07690691947937012, 0.692162275314331, 0.1497804820537567, 0.1497804820537567, 0.7489023804664612, 0.8257595300674438, 0.08678364753723145, 0.08941344916820526, 0.7927553653717041, 0.15947556495666504, 0.048536043614149094, 0.9310628175735474, 0.04541769623756409, 0.022708848118782043, 0.09244455397129059, 0.09244455397129059, 0.8320009708404541, 0.27957889437675476, 0.5791277289390564, 0.13978944718837738, 0.06104334071278572, 0.8546067476272583, 0.06104334071278572, 0.13727036118507385, 0.13727036118507385, 0.6863518357276917, 0.9458195567131042, 0.059113722294569016, 0.029556861147284508, 0.6904237270355225, 0.07671374827623367, 0.23653405904769897, 0.0910072922706604, 0.0910072922706604, 0.7280583381652832, 0.14846761524677277, 0.14846761524677277, 0.7423380613327026, 0.08224304020404816, 0.08224304020404816, 0.7401873469352722, 0.6812543869018555, 0.04752937704324722, 0.2693331241607666, 0.969322919845581, 0.02019422873854637, 0.02019422873854637, 0.38434430956840515, 0.4720017910003662, 0.14160053431987762, 0.13056278228759766, 0.19584417343139648, 0.6854546070098877, 0.9582304358482361, 0.019963134080171585, 0.019963134080171585, 0.762275755405426, 0.05025993660092354, 0.18847477436065674, 0.9686044454574585, 0.009883718565106392, 0.019767437130212784, 0.5666013956069946, 0.08407633751630783, 0.3472718298435211, 0.6811403036117554, 0.06273660808801651, 0.25692135095596313, 0.1246611475944519, 0.810297429561615, 0.06233057379722595, 0.07033830881118774, 0.8440597057342529, 0.07033830881118774, 0.5998671054840088, 0.12916846573352814, 0.271472692489624, 0.8727461099624634, 0.11929623037576675, 0.006278749089688063, 0.9483769536018372, 0.043271951377391815, 0.007211992051452398, 0.30638793110847473, 0.5978301167488098, 0.0971473976969719, 0.5681172609329224, 0.4104459583759308, 0.020021753385663033, 0.04750625044107437, 0.9026187658309937, 0.04750625044107437, 0.13965657353401184, 0.13965657353401184, 0.6982828974723816, 0.07379934936761856, 0.8117928504943848, 0.07379934936761856, 0.3208935558795929, 0.16044677793979645, 0.5214520692825317, 0.6967719793319702, 0.015746258199214935, 0.2873692214488983, 0.8522988557815552, 0.1322532743215561, 0.014694808050990105, 0.14641281962394714, 0.07320640981197357, 0.7320641279220581, 0.11203176528215408, 0.8402382731437683, 0.05601588264107704, 0.9226608872413635, 0.06437169015407562, 0.010728614404797554, 0.7724458575248718, 0.21671396493911743, 0.010728414170444012, 0.17044562101364136, 0.03408912569284439, 0.8010944128036499, 0.2517591118812561, 0.7238075137138367, 0.010489963926374912, 0.15077389776706696, 0.15077389776706696, 0.7538694739341736, 0.12707369029521942, 0.7878568768501282, 0.07624421268701553, 0.10738968849182129, 0.8591175079345703, 0.10738968849182129, 0.12257056683301926, 0.12257056683301926, 0.7354233860969543, 0.9380070567131042, 0.041689202189445496, 0.020844601094722748, 0.1015009731054306, 0.8120077848434448, 0.1015009731054306, 0.9737868905067444, 0.024652833119034767, 0.012326416559517384, 0.1418050229549408, 0.1418050229549408, 0.7090251445770264, 0.9633151888847351, 0.03360401839017868, 0.0056006694212555885, 0.5920098423957825, 0.3152714967727661, 0.09458145499229431, 0.35928449034690857, 0.29548630118370056, 0.3458532691001892, 0.3245134651660919, 0.5184051394462585, 0.15715432167053223, 0.7480413913726807, 0.17285418510437012, 0.08046659827232361, 0.780885636806488, 0.1294732689857483, 0.09305890649557114, 0.10224858671426773, 0.7157401442527771, 0.18125885725021362, 0.07141067832708359, 0.8569281697273254, 0.07141067832708359, 0.8515780568122864, 0.07996038347482681, 0.06796632707118988, 0.8819824457168579, 0.06108969449996948, 0.05727158859372139, 0.7137227654457092, 0.25917306542396545, 0.027910947799682617, 0.9388044476509094, 0.04542602226138115, 0.015142006799578667, 0.9785898923873901, 0.01322418823838234, 0.00661209411919117, 0.8825106620788574, 0.07267735153436661, 0.04152991250157356, 0.16077087819576263, 0.16077087819576263, 0.6430835127830505, 0.9274724721908569, 0.05796702951192856, 0.05796702951192856, 0.9376405477523804, 0.04167291149497032, 0.02083645574748516, 0.372580885887146, 0.15967753529548645, 0.47903257608413696, 0.6449771523475647, 0.1756533533334732, 0.17839792370796204, 0.09606148302555084, 0.8645533323287964, 0.09606148302555084, 0.057378191500902176, 0.8606728911399841, 0.08606728911399841, 0.03233138844370842, 0.9376102685928345, 0.03233138844370842, 0.8853294253349304, 0.04918496683239937, 0.0625990480184555, 0.07436496764421463, 0.8923795819282532, 0.07436496764421463, 0.06736034899950027, 0.8756844997406006, 0.06736034899950027, 0.9559330344200134, 0.024511104449629784, 0.024511104449629784, 0.14468207955360413, 0.04822735860943794, 0.8198651075363159, 0.20171239972114563, 0.7564215064048767, 0.025214049965143204, 0.9569138884544373, 0.03086818940937519, 0.03086818940937519, 0.9566863775253296, 0.034167371690273285, 0.017083685845136642, 0.2041381597518921, 0.13609211146831512, 0.680460512638092, 0.7861093878746033, 0.10992207378149033, 0.10326012969017029, 0.0892959013581276, 0.8036631345748901, 0.0892959013581276, 0.1735340654850006, 0.7635499238967896, 0.06941362470388412, 0.18018995225429535, 0.04504748806357384, 0.7658073306083679, 0.26299968361854553, 0.6537420749664307, 0.09017132222652435, 0.07136151939630508, 0.8028171062469482, 0.14272303879261017, 0.8542959690093994, 0.11865222454071045, 0.029663056135177612, 0.16102713346481323, 0.16102713346481323, 0.6441085338592529, 0.8753381967544556, 0.05406930297613144, 0.07160529494285583, 0.9512615203857422, 0.052847862243652344, 0.052847862243652344, 0.7962289452552795, 0.10296063870191574, 0.10296063870191574, 0.2220609486103058, 0.08882438391447067, 0.6661828756332397, 0.9130460619926453, 0.07966173440217972, 0.006127825938165188, 0.9459574222564697, 0.03503546118736267, 0.017517730593681335, 0.8353702425956726, 0.10278379917144775, 0.061468739062547684, 0.2664004862308502, 0.01665003038942814, 0.7201138138771057, 0.173322856426239, 0.7646596431732178, 0.06117277219891548, 0.023666266351938248, 0.9624282121658325, 0.00788875576108694, 0.0864233449101448, 0.8642334342002869, 0.0864233449101448, 0.9616548418998718, 0.018855977803468704, 0.018855977803468704, 0.2219688594341278, 0.1109844297170639, 0.665906548500061, 0.8956743478775024, 0.025737769901752472, 0.08236086368560791, 0.6256996393203735, 0.24588897824287415, 0.12733536958694458, 0.9591221809387207, 0.03093942627310753, 0.015469713136553764, 0.07899337261915207, 0.7899336814880371, 0.07899337261915207, 0.5313923954963684, 0.4270591139793396, 0.041556477546691895, 0.4161093533039093, 0.18386226892471313, 0.4064323902130127, 0.06722991913557053, 0.13445983827114105, 0.8067589998245239, 0.8416212201118469, 0.11439511924982071, 0.04085540026426315, 0.9422566890716553, 0.043825890868902206, 0.010956472717225552], \"Term\": [\"accept_charge\", \"accept_charge\", \"accept_charge\", \"advertise\", \"advertise\", \"advertise\", \"already\", \"already\", \"already\", \"amazing\", \"amazing\", \"amazing\", \"amp\", \"amp\", \"amp\", \"appear\", \"appear\", \"appear\", \"application\", \"application\", \"application\", \"arrive_fully\", \"arrive_fully\", \"arrive_fully\", \"average\", \"average\", \"average\", \"barely\", \"barely\", \"barely\", \"basic\", \"basic\", \"basic\", \"brand\", \"brand\", \"brand\", \"bright\", \"bright\", \"bright\", \"buy\", \"buy\", \"buy\", \"buying\", \"buying\", \"buying\", \"camera\", \"camera\", \"camera\", \"candle\", \"candle\", \"candle\", \"capacity\", \"capacity\", \"capacity\", \"cell\", \"cell\", \"cell\", \"charge\", \"charge\", \"charge\", \"charger\", \"charger\", \"charger\", \"circuitry\", \"circuitry\", \"circuitry\", \"claim\", \"claim\", \"claim\", \"come\", \"come\", \"come\", \"constantly\", \"constantly\", \"constantly\", \"controller\", \"controller\", \"controller\", \"cool\", \"cool\", \"cool\", \"country\", \"country\", \"country\", \"current\", \"current\", \"current\", \"day\", \"day\", \"day\", \"definately\", \"definately\", \"definately\", \"deliver\", \"deliver\", \"deliver\", \"describe\", \"describe\", \"describe\", \"device\", \"device\", \"device\", \"dim\", \"dim\", \"dim\", \"disappoint\", \"disappoint\", \"disappoint\", \"disappointed\", \"disappointed\", \"disappointed\", \"discharge\", \"discharge\", \"discharge\", \"discharge_rate\", \"discharge_rate\", \"discharge_rate\", \"dont_hold\", \"dont_hold\", \"dont_hold\", \"drop\", \"drop\", \"drop\", \"durable\", \"durable\", \"durable\", \"easy\", \"easy\", \"easy\", \"else\", \"else\", \"else\", \"eneloop\", \"eneloop\", \"eneloop\", \"exactly\", \"exactly\", \"exactly\", \"exchange\", \"exchange\", \"exchange\", \"expect\", \"expect\", \"expect\", \"expectation\", \"expectation\", \"expectation\", \"failure_rate\", \"failure_rate\", \"failure_rate\", \"fairy_light\", \"fairy_light\", \"fairy_light\", \"far\", \"far\", \"far\", \"fast_shippe\", \"fast_shippe\", \"fast_shippe\", \"fast_shipping\", \"fast_shipping\", \"fast_shipping\", \"first\", \"first\", \"first\", \"five_star\", \"five_star\", \"five_star\", \"flashlight\", \"flashlight\", \"flashlight\", \"frequently\", \"frequently\", \"frequently\", \"game_controller\", \"game_controller\", \"game_controller\", \"gaming\", \"gaming\", \"gaming\", \"garbage\", \"garbage\", \"garbage\", \"gave_star\", \"gave_star\", \"gave_star\", \"get\", \"get\", \"get\", \"go\", \"go\", \"go\", \"hard\", \"hard\", \"hard\", \"headlamp\", \"headlamp\", \"headlamp\", \"high\", \"high\", \"high\", \"high_drain\", \"high_drain\", \"high_drain\", \"high_hope\", \"high_hope\", \"high_hope\", \"highly_recommend\", \"highly_recommend\", \"highly_recommend\", \"hold_charge\", \"hold_charge\", \"hold_charge\", \"holds_charge\", \"holds_charge\", \"holds_charge\", \"holiday\", \"holiday\", \"holiday\", \"horrible\", \"horrible\", \"horrible\", \"hour\", \"hour\", \"hour\", \"house\", \"house\", \"house\", \"issue\", \"issue\", \"issue\", \"job\", \"job\", \"job\", \"kid\", \"kid\", \"kid\", \"last\", \"last\", \"last\", \"last_long\", \"last_long\", \"last_long\", \"life\", \"life\", \"life\", \"light\", \"light\", \"light\", \"likely\", \"likely\", \"likely\", \"load\", \"load\", \"load\", \"long\", \"long\", \"long\", \"lot\", \"lot\", \"lot\", \"love\", \"love\", \"love\", \"low\", \"low\", \"low\", \"make\", \"make\", \"make\", \"measure\", \"measure\", \"measure\", \"melt\", \"melt\", \"melt\", \"minimum\", \"minimum\", \"minimum\", \"minute\", \"minute\", \"minute\", \"money\", \"money\", \"money\", \"month\", \"month\", \"month\", \"morning\", \"morning\", \"morning\", \"motor\", \"motor\", \"motor\", \"mouse\", \"mouse\", \"mouse\", \"need\", \"need\", \"need\", \"night\", \"night\", \"night\", \"nimh\", \"nimh\", \"nimh\", \"okay\", \"okay\", \"okay\", \"output\", \"output\", \"output\", \"output_voltage\", \"output_voltage\", \"output_voltage\", \"overheat\", \"overheat\", \"overheat\", \"pair\", \"pair\", \"pair\", \"perhaps\", \"perhaps\", \"perhaps\", \"phone\", \"phone\", \"phone\", \"player\", \"player\", \"player\", \"plug\", \"plug\", \"plug\", \"power\", \"power\", \"power\", \"price\", \"price\", \"price\", \"product\", \"product\", \"product\", \"purchase\", \"purchase\", \"purchase\", \"put\", \"put\", \"put\", \"quality\", \"quality\", \"quality\", \"range\", \"range\", \"range\", \"really\", \"really\", \"really\", \"recharge\", \"recharge\", \"recharge\", \"rechargeable\", \"rechargeable\", \"rechargeable\", \"red\", \"red\", \"red\", \"remote\", \"remote\", \"remote\", \"replace\", \"replace\", \"replace\", \"reputation\", \"reputation\", \"reputation\", \"rotate\", \"rotate\", \"rotate\", \"save_money\", \"save_money\", \"save_money\", \"second\", \"second\", \"second\", \"seem\", \"seem\", \"seem\", \"self_discharge\", \"self_discharge\", \"self_discharge\", \"seller\", \"seller\", \"seller\", \"service\", \"service\", \"service\", \"set\", \"set\", \"set\", \"shelf_life\", \"shelf_life\", \"shelf_life\", \"ship\", \"ship\", \"ship\", \"solar\", \"solar\", \"solar\", \"solar_light\", \"solar_light\", \"solar_light\", \"state\", \"state\", \"state\", \"still_goe\", \"still_goe\", \"still_goe\", \"strong\", \"strong\", \"strong\", \"suck\", \"suck\", \"suck\", \"take\", \"take\", \"take\", \"temperature\", \"temperature\", \"temperature\", \"tenavolt\", \"tenavolt\", \"tenavolt\", \"terrible\", \"terrible\", \"terrible\", \"test\", \"test\", \"test\", \"thank\", \"thank\", \"thank\", \"thing\", \"thing\", \"thing\", \"three_time\", \"three_time\", \"three_time\", \"time\", \"time\", \"time\", \"tired\", \"tired\", \"tired\", \"try\", \"try\", \"try\", \"usage\", \"usage\", \"usage\", \"usb\", \"usb\", \"usb\", \"usb_port\", \"usb_port\", \"usb_port\", \"use\", \"use\", \"use\", \"value\", \"value\", \"value\", \"volt\", \"volt\", \"volt\", \"voltage\", \"voltage\", \"voltage\", \"voltage_drop\", \"voltage_drop\", \"voltage_drop\", \"wait\", \"wait\", \"wait\", \"walkie_talkie\", \"walkie_talkie\", \"walkie_talkie\", \"week\", \"week\", \"week\", \"well\", \"well\", \"well\", \"wish\", \"wish\", \"wish\", \"word\", \"word\", \"word\", \"work\", \"work\", \"work\", \"worth\", \"worth\", \"worth\", \"worthless\", \"worthless\", \"worthless\", \"year\", \"year\", \"year\", \"yet\", \"yet\", \"yet\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 2, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el156951403526943748323642794803\", ldavis_el156951403526943748323642794803_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el156951403526943748323642794803\", ldavis_el156951403526943748323642794803_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el156951403526943748323642794803\", ldavis_el156951403526943748323642794803_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2     -0.081357  0.008839       1        1  61.804134\n",
       "1      0.032759 -0.072525       2        1  24.936058\n",
       "0      0.048598  0.063686       3        1  13.259804, topic_info=         Term         Freq        Total Category  logprob  loglift\n",
       "263     value   240.000000   240.000000  Default  30.0000  30.0000\n",
       "61       work  1130.000000  1130.000000  Default  29.0000  29.0000\n",
       "8      charge  2591.000000  2591.000000  Default  28.0000  28.0000\n",
       "92       last   716.000000   716.000000  Default  27.0000  27.0000\n",
       "94       long   456.000000   456.000000  Default  26.0000  26.0000\n",
       "..        ...          ...          ...      ...      ...      ...\n",
       "24        get    33.522385   380.255981   Topic3  -5.6639  -0.4082\n",
       "213      come    31.831244   332.337128   Topic3  -5.7156  -0.3253\n",
       "445      take    31.030367   300.212677   Topic3  -5.7411  -0.2491\n",
       "57        try    30.090670   291.373474   Topic3  -5.7719  -0.2500\n",
       "79   recharge    29.604767   523.819946   Topic3  -5.7882  -0.8528\n",
       "\n",
       "[234 rows x 6 columns], token_table=      Topic      Freq           Term\n",
       "term                                \n",
       "3579      1  0.144394  accept_charge\n",
       "3579      2  0.144394  accept_charge\n",
       "3579      3  0.721968  accept_charge\n",
       "337       1  0.011205      advertise\n",
       "337       2  0.974819      advertise\n",
       "...     ...       ...            ...\n",
       "716       2  0.114395           year\n",
       "716       3  0.040855           year\n",
       "447       1  0.942257            yet\n",
       "447       2  0.043826            yet\n",
       "447       3  0.010956            yet\n",
       "\n",
       "[513 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 2, 1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(final_lda_model, corpus, id2word)\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
