{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ryankirkland/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords,wordnet\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/total_reviews.csv')\n",
    "cleaned = pd.read_csv('../data/cleaned_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B08267BBJT', 'B08268F6XN', 'B08267X3LH', 'B079JFK22D',\n",
       "       'B07QW531W2', 'B07TJTQDYG', 'B085HB8QVX', 'B07R2KK5P7',\n",
       "       'B07Y475MD3', 'B07F27PK2M', 'B085FZFVQV', 'B072R2SWXX',\n",
       "       'B0828KRQZ3', 'B0821ZNWKW', 'B07QV15B3W', 'B07MWYYDTM',\n",
       "       'B07RSJMS76', 'B07D1LMMDD', 'B0855TM65T', 'B07NTXYFBV',\n",
       "       'B086L3Q8YX', 'B085FT4YR1', 'B085XT3GTW', 'B07P9XZPYG',\n",
       "       'B0824WB5ST', 'B07HQ7QV7W', 'B083ZMYF55', 'B07FQD7PZ5',\n",
       "       'B07TWDR7VJ', 'B085DV7VZK', 'B086GTFGPP', 'B07Q6PZ2F4',\n",
       "       'B082W54KQK'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['asin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(pos_tag):\n",
    "\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ \n",
    "\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB \n",
    "\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    \n",
    "    else:\n",
    "        return wordnet.NOUN # default, return wordnet tag \"NOUN\"\n",
    "\n",
    "#Create a function to lemmatize tokens in the reviews\n",
    "def lemmatized_tokens(text):\n",
    "        text = text.lower()\n",
    "        pattern = r'\\b[a-zA-Z]{3,}\\b'                 \n",
    "        tokens = nltk.regexp_tokenize(text, pattern) # tokenize the text\n",
    "        tagged_tokens = nltk.pos_tag(tokens)  # a list of tuples (word, pos_tag)\n",
    "          \n",
    "        stop_words = stopwords.words('english')\n",
    "        new_stopwords = ['battery']  #customize extra stop_words\n",
    "        stop_words.extend(new_stopwords)\n",
    "        stop_words = set(stop_words)\n",
    "        \n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        # get lemmatized tokens                             #call function \"get_wordnet_pos\"\n",
    "        lemmatized_words=[wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(tag)) \n",
    "                  # tagged_tokens is a list of tuples (word, tag)\n",
    "                  for (word, tag) in tagged_tokens \\\n",
    "                  # remove stop words\n",
    "                  if word not in stop_words and \\\n",
    "                  # remove punctuations\n",
    "                  word not in string.punctuation]\n",
    "\n",
    "        return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = cleaned.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>product</th>\n",
       "      <th>date</th>\n",
       "      <th>verified</th>\n",
       "      <th>title</th>\n",
       "      <th>desc</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>rating</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>month_year</th>\n",
       "      <th>title_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B08267BBJT</td>\n",
       "      <td>AAA</td>\n",
       "      <td>2020-08-11</td>\n",
       "      <td>Verified Purchase</td>\n",
       "      <td>Didn't work, then worked, now don't work again</td>\n",
       "      <td>All I got in terms of use out of these batter...</td>\n",
       "      <td>Jasmine Carroll</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-08</td>\n",
       "      <td>Didn't work, then worked, now don't work again...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B08267BBJT</td>\n",
       "      <td>AAA</td>\n",
       "      <td>2020-07-30</td>\n",
       "      <td>Verified Purchase</td>\n",
       "      <td>These absolutely suck</td>\n",
       "      <td>I bought these for a wall mounted magnifying ...</td>\n",
       "      <td>Ashlee M.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-07</td>\n",
       "      <td>These absolutely suck I bought these for a wal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B08268F6XN</td>\n",
       "      <td>AA</td>\n",
       "      <td>2020-07-19</td>\n",
       "      <td>Verified Purchase</td>\n",
       "      <td>longer lasting battery for remote controller!!</td>\n",
       "      <td>i like the constant voltage and hopefully it ...</td>\n",
       "      <td>ARCHANGEL TROY</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-07</td>\n",
       "      <td>longer lasting battery for remote controller!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B08267BBJT</td>\n",
       "      <td>AAA</td>\n",
       "      <td>2020-07-18</td>\n",
       "      <td>Verified Purchase</td>\n",
       "      <td>Minimal plastic in packaging.</td>\n",
       "      <td>Just received these today, but Iâ€™m reviewing ...</td>\n",
       "      <td>ira</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-07</td>\n",
       "      <td>Minimal plastic in packaging. Just received th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B08267BBJT</td>\n",
       "      <td>AAA</td>\n",
       "      <td>2020-07-17</td>\n",
       "      <td>Verified Purchase</td>\n",
       "      <td>Not long enough battery life for a night hike</td>\n",
       "      <td>Shuts off suddenly in headlamp</td>\n",
       "      <td>T</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-07</td>\n",
       "      <td>Not long enough battery life for a night hike ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin product        date           verified  \\\n",
       "0  B08267BBJT     AAA  2020-08-11  Verified Purchase   \n",
       "1  B08267BBJT     AAA  2020-07-30  Verified Purchase   \n",
       "2  B08268F6XN      AA  2020-07-19  Verified Purchase   \n",
       "3  B08267BBJT     AAA  2020-07-18  Verified Purchase   \n",
       "4  B08267BBJT     AAA  2020-07-17  Verified Purchase   \n",
       "\n",
       "                                            title  \\\n",
       "0  Didn't work, then worked, now don't work again   \n",
       "1                           These absolutely suck   \n",
       "2  longer lasting battery for remote controller!!   \n",
       "3                   Minimal plastic in packaging.   \n",
       "4   Not long enough battery life for a night hike   \n",
       "\n",
       "                                                desc    reviewer_name  rating  \\\n",
       "0   All I got in terms of use out of these batter...  Jasmine Carroll     1.0   \n",
       "1   I bought these for a wall mounted magnifying ...        Ashlee M.     1.0   \n",
       "2   i like the constant voltage and hopefully it ...   ARCHANGEL TROY     5.0   \n",
       "3   Just received these today, but Iâ€™m reviewing ...              ira     5.0   \n",
       "4                     Shuts off suddenly in headlamp                T     3.0   \n",
       "\n",
       "   month  year month_year                                         title_desc  \n",
       "0      8  2020    2020-08  Didn't work, then worked, now don't work again...  \n",
       "1      7  2020    2020-07  These absolutely suck I bought these for a wal...  \n",
       "2      7  2020    2020-07  longer lasting battery for remote controller!!...  \n",
       "3      7  2020    2020-07  Minimal plastic in packaging. Just received th...  \n",
       "4      7  2020    2020-07  Not long enough battery life for a night hike ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt = cleaned.loc[0, 'title_desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmed_test = lemmatized_tokens(test_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['work',\n",
       " 'work',\n",
       " 'work',\n",
       " 'get',\n",
       " 'term',\n",
       " 'use',\n",
       " 'battery',\n",
       " 'three',\n",
       " 'day',\n",
       " 'use',\n",
       " 'two',\n",
       " 'additional',\n",
       " 'success',\n",
       " 'buy',\n",
       " 'bleed',\n",
       " 'aaa',\n",
       " 'battery',\n",
       " 'hop',\n",
       " 'compact',\n",
       " 'design',\n",
       " 'would',\n",
       " 'better',\n",
       " 'something',\n",
       " 'bulky',\n",
       " 'right',\n",
       " 'box',\n",
       " 'charge',\n",
       " 'light',\n",
       " 'green',\n",
       " 'indicate',\n",
       " 'fully',\n",
       " 'charge',\n",
       " 'try',\n",
       " 'use',\n",
       " 'couple',\n",
       " 'device',\n",
       " 'luck',\n",
       " 'go',\n",
       " 'return',\n",
       " 'friend',\n",
       " 'suggest',\n",
       " 'switch',\n",
       " 'charge',\n",
       " 'extension',\n",
       " 'cord',\n",
       " 'directly',\n",
       " 'wall',\n",
       " 'socket',\n",
       " 'think',\n",
       " 'trick',\n",
       " 'even',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'silly',\n",
       " 'try',\n",
       " 'battery',\n",
       " 'device',\n",
       " 'let',\n",
       " 'charge',\n",
       " 'overnight',\n",
       " 'plug',\n",
       " 'directly',\n",
       " 'wall',\n",
       " 'socket',\n",
       " 'work',\n",
       " 'work',\n",
       " 'well',\n",
       " 'three',\n",
       " 'day',\n",
       " 'later',\n",
       " 'device',\n",
       " 'stop',\n",
       " 'work',\n",
       " 'middle',\n",
       " 'high',\n",
       " 'power',\n",
       " 'usage',\n",
       " 'hair',\n",
       " 'trimmer',\n",
       " 'swap',\n",
       " 'battery',\n",
       " 'two',\n",
       " 'charge',\n",
       " 'entire',\n",
       " 'time',\n",
       " 'work',\n",
       " 'go',\n",
       " 'buy',\n",
       " 'regular',\n",
       " 'aaa',\n",
       " 'battery',\n",
       " 'device',\n",
       " 'go',\n",
       " 'back',\n",
       " 'work',\n",
       " 'fine',\n",
       " 'try',\n",
       " 'battery',\n",
       " 'device',\n",
       " 'work',\n",
       " 'seem',\n",
       " 'work',\n",
       " 'enough',\n",
       " 'convince',\n",
       " 'work',\n",
       " 'stop',\n",
       " 'work',\n",
       " 'often',\n",
       " 'leave',\n",
       " 'feedback',\n",
       " 'product',\n",
       " 'felt',\n",
       " 'important',\n",
       " 'say',\n",
       " 'would',\n",
       " 'recommend',\n",
       " 'anyone',\n",
       " 'buy',\n",
       " 'battery',\n",
       " 'totally',\n",
       " 'faulty',\n",
       " 'least',\n",
       " 'completely',\n",
       " 'inconsistent',\n",
       " 'good',\n",
       " 'buying',\n",
       " 'disposable']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to build the optimal LDA model\n",
    "def optimal_lda_model(df, review_colname):\n",
    "    '''\n",
    "    INPUTS:\n",
    "        df_review - dataframe that contains the reviews\n",
    "        review_colname: name of column that contains reviews\n",
    "        \n",
    "    OUTPUTS:\n",
    "        lda_tfidf - Latent Dirichlet Allocation (LDA) model\n",
    "        dtm_tfidf - document-term matrix in the tfidf format\n",
    "        tfidf_vectorizer - word frequency in the reviews\n",
    "        A graph comparing LDA Model Performance Scores with different params\n",
    "    '''\n",
    "    docs_raw = df[review_colname].tolist()\n",
    "\n",
    "    #************   Step 1: Convert to document-term matrix   ************#\n",
    "\n",
    "    #Transform text to vector form using the vectorizer object \n",
    "    tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                    stop_words = 'english',\n",
    "                                    lowercase = True,\n",
    "                                    token_pattern = r'\\b[a-zA-Z]{3,}\\b', # num chars > 3 to avoid some meaningless words\n",
    "                                    max_df = 0.9,                        # discard words that appear in > 90% of the reviews\n",
    "                                    min_df = 10)                         # discard words that appear in < 10 reviews    \n",
    "\n",
    "    #apply transformation\n",
    "    tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())\n",
    "\n",
    "    #convert to document-term matrix\n",
    "    dtm_tfidf = tfidf_vectorizer.fit_transform(docs_raw)  \n",
    "\n",
    "    print(\"The shape of the tfidf is {}, meaning that there are {} {} and {} tokens made through the filtering process.\".\\\n",
    "              format(dtm_tfidf.shape,dtm_tfidf.shape[0], review_colname, dtm_tfidf.shape[1]))\n",
    "\n",
    "    \n",
    "    #*******   Step 2: GridSearch & parameter tuning to find the optimal LDA model   *******#\n",
    "\n",
    "    # Define Search Param\n",
    "    search_params = {'n_components': [5, 10, 15, 20, 25, 30], \n",
    "                     'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "    # Init the Model\n",
    "    lda = LatentDirichletAllocation()\n",
    "\n",
    "    # Init Grid Search Class\n",
    "    model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "    # Do the Grid Search\n",
    "    model.fit(dtm_tfidf)\n",
    "\n",
    "\n",
    "    #*****  Step 3: Output the optimal lda model and its parameters  *****#\n",
    "\n",
    "    # Best Model\n",
    "    best_lda_model = model.best_estimator_\n",
    "\n",
    "    # Model Parameters\n",
    "    print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "    # Log Likelihood Score: Higher the better\n",
    "    print(\"Model Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "    # Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "    print(\"Model Perplexity: \", best_lda_model.perplexity(dtm_tfidf))\n",
    "\n",
    "\n",
    "    #***********   Step 4: Compare LDA Model Performance Scores   ***********#\n",
    "\n",
    "    #Get Log Likelyhoods from Grid Search Output\n",
    "    gscore=model.fit(dtm_tfidf).cv_results_\n",
    "    n_topics = [5, 10, 15, 20, 25, 30]\n",
    "\n",
    "    log_likelyhoods_5 = [gscore['mean_test_score'][gscore['params'].index(v)] for v in gscore['params'] if v['learning_decay']==0.5]\n",
    "    log_likelyhoods_7 = [gscore['mean_test_score'][gscore['params'].index(v)] for v in gscore['params'] if v['learning_decay']==0.7]\n",
    "    log_likelyhoods_9 = [gscore['mean_test_score'][gscore['params'].index(v)] for v in gscore['params'] if v['learning_decay']==0.9]\n",
    "\n",
    "    # Show graph\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
    "    plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
    "    plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
    "    plt.title(\"Choosing Optimal LDA Model\")\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Log Likelyhood Scores\")\n",
    "    plt.legend(title='Learning decay', loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return gscore, best_lda_model, dtm_tfidf, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryankirkland/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1798: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. <class 'numpy.int64'> 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the tfidf is (4745, 1120), meaning that there are 4745 title_desc and 1120 tokens made through the filtering process.\n",
      "Best Model's Params:  {'learning_decay': 0.5, 'n_components': 5}\n",
      "Model Log Likelihood Score:  -25759.51499442464\n",
      "Model Perplexity:  1279.1050702123528\n"
     ]
    }
   ],
   "source": [
    "gscore, best_lda_model, dtm_tfidf, tfidf_vectorizer = optimal_lda_model(cleaned, 'title_desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
